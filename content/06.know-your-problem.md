## Tip 4: Know your data and your question {#know-your-problem}

Having a well defined scientific question and a clear analysis plan is crucial for carrying out a successful deep learning project.
Just like it would be inadvisable to step foot in a laboratory and begin experiments without having a defined endpoint, a deep learning project should not be undertaken without preparation.
Foremost, it is important to assess if a dataset exists that can answer the biological question of interest for the given deep learning model; obtaining said data and associated metadata and reviewing the study protocol should be pursued as early on in the project as possible.
A publication or resource might purportedly offer a dataset that seems to be a good fit to test your hypothesis, but the act of obtaining it can reveal numerous problems.
It may be unstructured when it is supposed to be structured, crucial metadata such as sample stratification are missing, or the usable sample size is different than what is reported.
Data collection should be documented or a data collection protocol should be created and specified in the project documentation.
Information such as the resource used, the date downloaded, and the version of the dataset, if any, will help minimize operational confusion and will allow for transparency during the publication process.

Once the dataset is obtained, it is easy to begin analysis without a good understanding of the study design, namely why the data was collected and how.
Metadata has been standardized in many fields and can help with this (for example, see [@doi:10.1038/ng1201-365]), but if at all possible, seek out a subject matter expert who has experience with this type of data.
Receiving first-hand knowledge of the “gotchas" of a dataset will minimize the amount of guesswork and increase the success rate of a deep learning project.
For example, if the main reason why the data was collected was to test the impact of an intervention, then it may be the case that a randomized controlled trial was performed.
However, it is not always possible to perform a randomized trial for ethical or practical reasons.
Therefore, an observational study design is often considered, with the data either prospectively or retrospectively collected.
In order to ensure similar distributions of important characteristics across study groups in the absence of randomization, individuals may be matched based on age, gender, or weight.
Study designs will often have different assumptions and caveats, and these cannot be ignored during a data analysis.
Many datasets are now passively collected or do not have a specific design, but even in this case it is important to know how individuals or samples were treated.
Samples originating from the same study site, oversampling of ethnic groups or zip codes, and sample processing differences are all sources of variation that need to be accounted for.

Systematic biases, which can be induced by confounding variables, for example, can lead to artifacts or so-called "batch effects."
As a consequence, models may learn to rely on correlations that are irrelevant in the scientific context of the study and may result in misguided predictions and misleading conclusions [@doi:10.1038/nrg2825].
Other study design considerations that should not be overlooked include knowing whether a study involves biological or technical replicates or both.
For example, are some samples collected from the same individuals at different time points?
Are those time points before and after some treatment?
If one assumes that all the samples are independent but that is in fact not the case, a variety of issues may arise, including having a lower effective sample size than expected.
As described in [Tip 1](#concepts), unsupervised learning and other exploratory analyses can be identify such biases in these datasets prior to applying the deep learning model.

In general, deep learning has an increased tendency for overfitting, compared to classical methods, due to the large number of parameters being estimated, making issues of adequate sample size even more important (see [Tip 7](#overfitting)).
For a large dataset, overfitting may not be a concern, but the modeling power of deep learning may lead to more spurious correlations and thus incorrect interpretation of results (see [Tip 9](#interpretation)).
It is important to note that molecular or imaging datasets often require appropriate clinical or demographic data to support robust analyses; this must always be balanced with the need to protect patient privacy (see [Tip 10](#privacy)).
Looking at these annotations can also clarify the study design (for example, by seeing if all the individuals are adolescents or women) or at least help the analyst employing deep learning to know what questions to ask.

Data simulation is a powerful approach to creating additional data for model testing.
In data simulation, a model is used to learn the true distribution of a training set for the purpose of creating new data points.
Simulations should be performed under reasonable assumptions since the goal would be to identify useful model architectures and hyperparameters, and simulated datasets can be used to verify the correctness of a model’s implementation.
To accurately test the performance of the model, it is important that simulated datasets be generated for a range of parameters.
For example, varying the parameters to violate the model's assumptions can test the sensitivity of the model's performance.

Basically, thoroughly study your data and ensure that you understand its context and peculiarities _before_ jumping into deep learning.
