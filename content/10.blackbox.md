## Tip 8: Deep learning models can be made more transparent {#blackbox}

While model interpretability is a broad concept, in much of the machine learning literature it refers to the ability to identify the discriminative features that influence or sway the predictions.
In certain cases, the goal behind interpretation is to understand the underlying data generating processes and biological mechanisms [@doi:10.3390/biom10030454].
In other cases, the goal is to understand why a model made the prediction that it did for a specific example or set of examples.
Machine learning models vary widely in terms of interpretability: some are fully transparent while others are considered "black-boxes" that make predictions with little ability to examine why.
Logistic regression and decision tree models are generally considered interpretable [@doi:10.1007/978-1-4939-7756-7_16].
In contrast, deep neural networks are often considered among the most difficult to interpret naively because they can have many parameters and non-linear relationships.

Knowing which of the input variables influences a model's predictions, and potentially in what ways, can help with the application or extrapolation of machine learning models.
This is particularly important in biomedicine.
Subsequent decision making often requires human input, and models are employed with the hope of better understanding why relationships exist in the first place.
Furthermore, while prediction rules can be derived from high-throughput molecular datasets, most affordable clinical tests still rely on lower-dimensional measurements of a limited number of biomarkers.
Therefore, it is often unclear how to translate the predictive capacity of deep learning models that encompass non-linear relationships between countless input variables into clinically digestible terms.
As a result, selecting which biomarkers to use for decision making remains an important modeling and interpretation challenge.
In fact, many authors attribute a lower uptake of deep learning tools in healthcare to interpretability challenges [@doi:10.1109/JBHI.2016.2636665; @doi:10.1038/s41551-018-0315-x].
Nonetheless, strategies to interpret both machine learning and deep learning models are rapidly emerging, and the literature on the topic is growing exponentially [@arxiv:2001.02522].
Instead of recommending specific methods for either deep learning-specific or general-purpose model interpretation, we suggest consulting a freely available and continually updated textbook [@url:https://christophm.github.io/interpretable-ml-book/].
