## Tip 8: Your DL models can be more transparent {#blackbox}

Model interpretability is a broad concept.
In certain cases, the goal behind interpretation is to understand the underlying data generating processes while in other cases the goal is to understand why a model made the prediction that it did for a specific example or set of examples.
In much of the ML literature, including in our guidelines,  the concept of model interpretability refers to the ability to identify the discriminative features that influence or sway the predictions.
ML models vary widely in terms of interpretability: some are fully transparent while others are considered to be "black-boxes" that make predictions with little ability to examine why.
Logistic regression and decision tree models are generally considered interpretable, while deep neural networks are often considered among the most difficult to interpret because they can have many parameters and non-linear relationships. 

Model interpretability is particularly important in biomedicine, where subsequent decision making often requires human input.
For example, while prediction rules can be derived from high-throughput molecular datasets, most affordable clinical tests rely on lower dimensional measurements of a limited number of biomarkers.
Selecting those biomarkers to support decision making is an important modeling and interpretation challenge.
Many authors attribute a lower uptake of DL tools in healthcare to interpretability challenges [@doi:10.1109/JBHI.2016.2636665; @doi:10.1038/s41551-018-0315-x]. 
Strategies to interpret both ML and DL models are rapidly emerging, and the literature on the topic is growing at an exponential rate [@arxiv:2001.02522].
Therefore, instead of recommending specific methods for either DL-specific or general-purpose model interpretation, we suggest consulting [@url:https://christophm.github.io/interpretable-ml-book/] which is freely available and continually updated.

Model interpretation is an open, active area of research.
It is becoming more feasible to interpret models with many parameters and non-linear relationships, but in many cases simpler models remain substantially easier to interpret than more complex ones.
When deciding on a machine learning approach and model architecture, consider an interpretability versus accuracy tradeoff.
A challenge in considering this tradeoff is that the extent to which one trades interpretability for accuracy depends on the problem itself.
When the features provided to the model are already highly relevant to the task at hand, a simpler, interpretable model that gives up only a little performance when compared to a very complex one more useful in many settings.
On the other hand, if features must be combined in complex ways to be meaningful for the task, the performance difference of a model capable of capturing that structure may outweigh the interpretability costs.
An appropriate choice can only be made after careful consideration, which often includes estimating the performance of a simple, linear model that serves as a [baseline](#baselines).
In cases where models are learned from high-throughput datasets, a small subset of features in the dataset may be strongly correlated with the complex combination of the larger feature set defined from the deep learning model.
In this case, this more limited number of features can themselves be used in the subsequent simplified model to further enhance interpretability of the model.
This feature reduction can be essential to defining biomarker panels that enable clinical applications.
