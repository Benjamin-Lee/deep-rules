## Tip 3: Understand the complexities of training deep neural networks {#complexities}

Correctly training deep neural networks is a non-trivial process, as there are many different options and potential pitfalls at every stage.
To get good results, you must expect to train many networks across a range of different parameter and hyperparameter settings.
Such training can be made more difficult by the demanding nature of these deep networks, which often require extensive computing infrastructure and optimization in order to achieve state-of-the-art performance [@doi:10.1109/JPROC.2017.2761740].
Furthermore, this experimentation is often noisy, which necessitates increased repitition and exacerbates the organizational challenges inherent to DL.
On the whole, all code, random seeds, parameters, and results must be carefully corralled using general good coding practices (e.g. version control [@doi:10.1371/journal.pcbi.1004947], continuous integration etc.) in order to be effective and interpretable.
This organization is also fundamental to the efficient sharing and reproducibility of research work [@doi:10.1371/journal.pcbi.1003285; @arxiv:1810.08055], and to the ability to keep models up to date as new data becomes available.

One specific reproducibility pitfall that is often missed in the application of deep learning is the default use of non-deterministic algorithms by CUDA/CuDNN backends when using GPUs.
That is, the CUDA/CuDNN architectures that facilitate the parallelized computing that power state-of-the-art DL often use algorithms by default that produce different outcomes from iteration to iteration.
Therefore, achieving reproducibility in this context requires explicitly specifying the use of deterministic algorithms (which are typically available within your DL library), which is distinct from the setting of random seeds that typically achieve reprocubility by controlling pseudorandom deterministic procedures such as shuffling and initialization [@url:https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#reproducibility].

Similar to the suggestions in [Tip 2](#baselines) about starting with simpler models, try to start with a relatively small network and then increase the size and complexity as needed.
This can help to prevent practitioners from wasting signficant time and resources on running highly complex models that feature numerous unresolved problems. 
Again, beware of the choices that are being made implicitly (_i.e._ by default settings) by your framework of choice (e.g. choice of optimization algorithm), as these seemingly trivial specifics can actually have signficant effects on model performance.
For example, adaptive methods often lead to faster convergence during training, but may lead to worse generalization performance on independent datasets [@url:https://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning]).
These nuanced elements are easy to overlook, but it is critical to carefully consider them and to evaluate their potential impact (see [Tip 6](#hyperparameters)).

In short, use smaller and simpler networks to enable faster prototyping and follow general software development best practices to maximize reproducibility.
