## Tip 1: Decide whether deep learning is appropriate for your problem {#appropriate}

In recent years, the number of publications implementing deep learning in biology have risen tremendously.
This is likely driven by deep learning's usefulness across a range of scientific questions and data modalities, and can contribute to the appearance of deep learning as a panacea for modeling problems.
Indeed, neural networks are universal function approximators, and derive tremendous power from this theoretical capacity to learn any function [@doi:10.1007/BF02551274; @tag:hornik-approximation].
The immense power and popularity of deep learning begs the question of why these models are not used universally as the clear and best choice for all prediction tasks.

The reason is simple: in reality, deep learning is not suited to every situation.
Training deep learning models requires a significant amount of data, computing power, and expertise.
In some areas of biology where data collection is thoroughly automated, such as DNA sequencing, large amounts of quality data may be available.
For other areas which rely on manual data collection, there may not be enough data to effectively train models.
Though there are methods to increase the amount of training data, such as data augmentation (in which existing data is slightly manipulated to yield "new" samples) and weak supervision (in which simple labeling heuristics are combined to produce noisy, probabilistic labels) [@arxiv:1605.07723v3], these methods cannot overcome a complete shortage of data.
In the context of supervised classification, deep learning should be considered for datasets with at least one hundred samples per class [@arxiv:1511.06348] as a rule of thumb, though in all cases it is best suited to cases when datasets contain orders of magnitude more samples.

Furthermore, training deep learning models can be very demanding, often requiring extensive computing infrastructure and patience to achieve state-of-the-art performance [@doi:10.1109/JPROC.2017.2761740].
In some deep learning contexts, such as generating human-like text, state-of-the-art models have over one hundred billion parameters [@arxiv:2005.14165].
Training such large models from scratch can be a costly and time-consuming undertaking [@arxiv:1906.02243].
Luckily, most deep learning research in biology will not require nearly as much computation, though it usually requires more than can be done feasibly on an individual consumer-grade device.
Specialized hardware such as discrete graphics processing units (GPUs) or custom deep learning accelerators can dramatically reduce the time and cost required to train models, but this hardware is not universally accessible.
Currently, both GPU- and deep learning-optimized accelerator-equipped servers can be rented from cloud providers, though working with these servers adds additional cost and complexity.
As deep learning becomes more popular, these accelerators are likely to be more broadly available (for example, recent-generation iPhones already have such hardware).
In contrast, traditional machine learning training can often be done on a laptop (or even a \$5 computer [@arxiv:1809.00238]) in seconds to minutes.

Beyond the necessity for greater data and computational capacity in deep learning, building and training deep learning models generally requires more expertise than traditional machine learning models.
Currently, there are several competing programming frameworks for deep learning, such as Tensorflow [@arxiv:1603.04467] and PyTorch [@arxiv:1912.01703].
These frameworks allow users to create and deploy entirely novel model architectures and are widely used in deep learning research as well as in industrial applications.
This flexibility combined with the rapid development of the deep learning field has resulted in large, complex frameworks that can be daunting to new users.
For readers new to software development but experienced in biology, gaining computational skills while interfacing with such complex industrial-grade tools can be a challenge.
An advantage of machine learning over deep learning is that currently there are more tools capable of automating the model selection and training process.
Automated machine learning (AutoML) tools such as TPOT [@doi:10.1007/978-3-319-31204-0_9], which is capable of using genetic programming to optimize machine learning pipelines, and Turi Create [@https://github.com/apple/turicreate], a task-oriented machine learning and deep learning framework which automatically tests multiple machine learning models when training, allow users to achieve competitive performance with only a few lines of code.
Luckily, there are efforts underway to reduce the expertise required to build and use deep learning models.
Indeed, both TPOT and Turi Create, as well as other tools such as AutoKeras [@arxiv:1806.10282], are capable of abstracting away much of the programming required for "standard" deep learning tasks.
Projects such as Keras [@https://keras.io], a high-level interface for TensorFlow, make it relatively straightforward to design and test custom deep learning architectures.
In the future, projects such as these are likely to bring deep learning experimentation within reach to even more researchers.

There are some types of problems in which using deep learning is strongly indicated over machine learning.
Assuming a sufficient quantity of quality data is available, applications such as computer vision and natural language processing are likely to benefit from deep learning.
In fact, these areas were the first to see significant breakthroughs through the application of deep learning [@doi:10.1145/3065386] during the recent deep learning revolution."
For example, Ferreira et al. used deep learning to recognize individual birds from images [@doi:10.1111/2041-210X.13436].
This problem was historically difficult but, by combining automatic data collection using RFID tags with data augmentation and transfer learning (explained in [Tip 5](#architecture)), the authors were able to use deep learning to achieve 90% accuracy in several species.
Other areas include generative models, in which new samples are able to be created based on the training data, and reinforcement learning, in which agents are trained to interact with their environments.
In general, before using deep learning, investigate whether similar problems (including analogous ones in other domains) have been solved successfully using deep learning.

Depending on the amount and the nature of the available data, as well as the task to be performed, deep learning may not always be able to outperform conventional methods.
As an illustration, Rajkomar et al. [@doi:10.1038/s41746-018-0029-1] found that simpler baseline models achieved performance comparable with that of deep learning in a number of clinical prediction tasks using electronic health records, which may be a surprise to many.
Another example is provided by Koutsoukas et al., who benchmarked several traditional machine learning approaches against deep neural networks for modeling bioactivity data on moderately sized datasets [@doi:10.1186/s13321-017-0226-y].
The researchers found that while well tuned deep learning approaches generally tend to outperform conventional classifiers, simple methods such as Naive Bayes classification tend to outperform deep learning as the noise in the dataset increases.
Similarly, Chen et al. [@doi:10.1038/s41746-019-0122-0] tested deep learning and a variety of traditional machine learning methods such as logistic regression and random forests on five different clinical datasets, finding that the non deep learning methods matched or exceeded the accuracy of the deep learning model in all cases while requiring an order of magnitude less training time.

In conclusion, deep learning is a tool and, like any other tool, must be used after consideration of its strengths and weaknesses for the problem at hand.
Once settled upon deep learning as a potential solution, practitioners should follow the scientific method and compare its performance to traditional methods, as we will see next.
