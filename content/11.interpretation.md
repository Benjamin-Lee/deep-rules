## Tip 9: Don't over-interpret predictions {#interpretation}

After training an accurate deep learning model, it is natural to want to use it to deduce relationships and inform scientific findings.
However, be careful to interpret the model's predictions correctly.
Given that deep learning models can be difficult to interpret intuitively, there is often a temptation to over-interpret the predictions in indulgent or inaccurate ways.
In accordance with the classic statistical saying "correlation doesn't imply causation," predictions by deep learning models rarely provide causal relationships.
Accurately predicting an outcome does not mean that a causal mechanism has been learned, even when predictions are extremely accurate.
In a poignant example, authors evaluated the capacities of several models to predict the probability of death for patients with pneumonia admitted to an intensive care unit [@tag:predicting-pneumonia-mortality; @doi:10.1145/2783258.2788613].
The neural network model achieved the best predictive accuracy.
However, after fitting a rule-based model to understand the relationships inherent to their data better, the authors discovered that the hospital data implied the rule "$\text{HasAsthma}(x) \Rightarrow \text{LowerRisk}(x)$."
This rule contradicts medical understanding, as having asthma does not make pneumonia better!
Nonetheless, the data supported this rule, as pneumonia patients with a history of asthma tended to receive more aggressive care.
The neural network had, therefore, also learned to make predictions according to this rule despite the fact that it has nothing to do with causality or mechanism.
Therefore, it would have been disastrous to guide treatment decisions according to the predictions of the neural network, even though the neural network had high predictive accuracy.

Avoid over-interpreting deep learning models by viewing them for what they are: complex statistical models trained on high dimensional data.
If causal inference is desired, special techniques for causal inference are required [@doi:10.1038/s42256-020-0218-x].
