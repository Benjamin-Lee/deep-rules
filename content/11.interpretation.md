## Tip 9: Don't over-interpret predictions {#interpretation}

Deep learning models can make predictions with high accuracy, but we need to take care to correctly interpret these predictions.
We know that the basic tenets of machine learning also apply to deep learning ([Tip 1](#concepts)), but because deep models can be difficult to interpret intuitively, there is a temptation to anthropomorphize deep models.
We must resist this temptation.

A common saying in statistics classes is "correlation doesn't imply causality".
While we know that accurately predicting an outcome doesn't imply learning the causal mechanism, it can be easy to forget this lesson when the predictions are extremely accurate.
A poignant example of this lesson is [@tag:predicting-pneumonia-mortality; @doi:10.1145/2783258.2788613].
In this study, the authors evaluated the capacities of several models to predict the probability of death for patients admitted to an intensive care unit with pneumonia.
Unsurprisingly, the neural network model achieved the best predictive accuracy.
However, after fitting a rule-based model, the authors discovered that the hospital data implied the rule "HasAsthma(x) => LowerRisk(x)".
This rule contradicts medical understanding - having asthma doesn't make pneumonia better!
This rule was supported by the data (pneumonia patients with a history of pneumonia tended to receive more aggressive care), so the neural network also learned to make predictions according to this rule.
Guiding treatment decisions according to the predictions of the neural network would have been disastrous, even though the neural network had high predictive accuracy.

To trust the reasoning and scientific conclusions of deep learning models, combine knowledge of the data ([Tip 4](#know-your-problem)) with inspection of the model ([Tip 8](#blackbox)).
