## Tip 1: Concepts that apply to machine learning also apply to deep learning {#concepts}

While DL has proven to be an extremely powerful paradigm that is capable of outperforming “traditional” machine learning approaches in certain contexts, it is not immune to the many limitations inherent to machine learning.

Therefore, many best practices for machine learning also apply to deep learning.

Like all computational methods, deep learning should be applied in a systematic manner, and should include rigorous testing and reproducibility.

When training and testing model performance, DL practitioners should select datasets that are relevant to the problem at hand, as incongruent data can hamper performance or lead to spurious conclusions.

For example, supervised deep learning for phenotype prediction should be applied to datasets that contain large numbers of representative samples from all phenotypes to be predicted.




Investigators should also consider whether datasets they use are potentially confounded, biased, and/or of low quality, as such datasets are likely to produce models of questionable veracity and limited generalizability.

One strategy for identifying such data quality issues is to utilize unsupervised learning (i.e. clustering) and data visualization techniques to explore the structure of the data.

This can serve as a critical first step in assessing data quality before any deep learning model is applied, and can help to reveal biases and technical artifacts. 

In some cases, these initial exploratory analyses can identify biases from known sources (e.g. sample processing), and these biases can be corrected through preprocessing techniques that help to increase the accuracy of subsequent deep learning modeling.

With regards to the possiblity of confounding, researchers should consider the extent to which the outcome of interest is likely to be predictable from the input data.

For example, if robust heritability estimates for a phenotype suggest that genetic contribution is modest, then researchers might reasonably be skeptical about a deep learning model that uses genetic data to predict the phenotype with very high accuracy.

Such a model may be capturing signal that underlies the phenotype but that is unrelated to genetic mechanisms.

One possible explanation for such a case might be that people with similar genetic markers may have shared exposures, which would lead to confonding between the genetic data the DL model is training on, and the shared exposures influencing the phenotypic outcome data.

Confounding can also potentially explain situations were DL models produce very high accuracies despite an input dataset featuring only modest agreement between predcitor variables (i.e. modest inter-rater reliability). This is something that researchers should probe before reporting unrealistic accuracy measures,




Since using a test set more than once can lead to biased estimates of model generalization and performance [@arxiv:1811.12808; @doi:10.1162/089976698300017197],
supervised DL models should be trained, tuned, and tested on non-overlapping datasets.

That is, the data used for testing should unequivocally be set aside, and should only be used to evaluate the final model after all tuning steps are completed.

Also, many conventional metrics for classification (e.g. area under the receiver operating characteristic curve, or AUROC/AUC/ROC) have limited utility in cases of extreme class imbalance [@pmid:25738806] (i.e. when there is severe imbalance between the classes one is trying to predict).

To address this, model performance should be evaluated with a carefully picked panel of relevant metrics that make minimal assumptions about the composition of the testing data [@doi:10.1021/acs.molpharmaceut.7b00578]. Furthermore, metrics that are most directly applicable to the task at hand should be given specific consideration.

On the whole, we strongly recommend that individuals with minimal machine learning familiarity review a general machine learning guide, such as [@doi:10.1186/s13040-017-0155-3], before moving to utilize deep learning. Since data limitations can be difficult to identify, and given DL's oft unclear inner mechanics, is it imperative that DL practioners maintain an awareness of and caution towards potential issues.
