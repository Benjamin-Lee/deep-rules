## Tip 1: Concepts that apply to machine learning also apply to deep learning {#concepts}

Deep learning is a distinct subfield of machine learning, but it is still a subfield.
DL has proven to be an extremely powerful paradigm capable of outperforming “traditional” machine learning approaches in certain contexts, but it is not immune to the many limitations inherent to machine learning.
Many best practices for machine learning also apply to deep learning.
Like all computational methods, deep learning should be applied in a systematic manner that is reproducible and rigorously tested.

Those developing deep learning models should select datasets to train and test model performance that are relevant to the problem at hand; non-salient data can hamper performance or lead to spurious conclusions. For example, supervised deep learning for phenotype prediction should be applied to datasets that contain large numbers of representative samples from all phenotypes to be predicted.
Biases in testing data can also unduly influence measures of model performance, and it may be difficult to directly identify confounders from the model.
Investigators should consider the extent to which the outcome of interest is likely to be predictable from the input data and begin by thoroughly inspecting the input data.
Suppose that there are robust heritability estimates for a phenotype that suggest that the genetic contribution is modest but a deep learning model predicts the phenotype with very high accuracy.
The model may be capturing signal unrelated to genetic mechanisms underlying the phenotype.
In this case, a possible explanation is that people with similar genetic markers may have shared exposures.
This is something that researchers should probe before reporting unrealistic accuracy measures.
A similar situation can arise with tasks for which inter-rater reliability is modest but deep learning models produce very high accuracies.
When coupled with imprudence, datasets that are confounded, biased, skewed, or of low quality will produce models of dubious performance and limited generalizability. Data exploration with unsupervised learning and data visualization can reveal the biases and technical artifacts in these datasets, providing a critical first step to assessing data quality before any deep learning model is applied. In some cases, these analyses can identify biases from known technical artifacts or sample processing which can be corrected through preprocessing techniques to support more accurate application of deep leaning models for subsequent prediction or feature identifaction problems from those datasets.

Using a test set more than once will lead to biased estimates of the generalization performance  [@arxiv:1811.12808; @doi:10.1162/089976698300017197].
Deep supervised learning models should be trained, tuned, and tested on non-overlapping datasets.
The data used for testing should be locked and only used one-time for evaluating the final model after all tuning steps are completed.
Also, many conventional metrics for classification (e.g. area under the receiver operating characteristic curve or AUROC) have limited utility in cases of extreme class imbalance [@pmid:25738806].
Model performance should be evaluated with a carefully picked panel of relevant metrics that make minimal assumptions about the composition of the testing data [@doi:10.1021/acs.molpharmaceut.7b00578], with particular consideration given to metrics that are most directly applicable to the task at hand.

Extreme cases warrant testing the robustness of the model and metrics on simulated data for which the ground truth is known. To accurately test the performance of the model, it is important that simulated datasets be generated for a range of parameters.
Simulated datasets generated from parameters that are consistent with the assumptions of the deep learning models can be used to verify the correctness of the model’s implementation. Further varying the parameters to test variation and violations of model assumptions can further test the sensitivity of performance to those assumptions. Generative models based upon sample training sets can provide important tools to generate large cohorts of simulated datasets that are representative of the real world problems to which the machine learning prediction models will be applied.

In summary, if you are not familiar with machine learning, review a general machine learning guide such as [@doi:10.1186/s13040-017-0155-3] before diving right into deep learning.
