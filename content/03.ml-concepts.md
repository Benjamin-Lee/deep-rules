## Tip 1: Concepts that apply to machine learning also apply to deep learning {#concepts}

Deep learning is a distinct subfield of machine learning, but it is still a subfield.
DL has proven to be an extremely powerful paradigm capable of outperforming “traditional” machine learning approaches in certain contexts, but it is not immune to the many limitations inherent to machine learning.
Many best practices for machine learning also apply to deep learning.
Like all computational methods, deep learning should be applied in a systematic manner that is reproducible and rigorously tested.

You should select data that are relevant to the problem at hand; non-salient data can hamper performance or lead to spurious conclusions.
Biases in testing data can also unduly influence measures of model performance, and it may be difficult to directly identify confounders from the model.
You should consider the extent to which the outcome of interest is likely to be predictable from the input data and begin by throughly inspecting the input data.
Suppose that there are robust heritability estimates for a phenotype that suggest that the genetic contribution is modest but a deep learning model predicts the phenotype with very high accuracy.
The model may be capturing signal unrelated to genetic mechanisms underlying the phenotype.
In this case, a possible explanation is that people with similar genetic markers may have shared exposures.
This is something that researchers should probe before reporting unrealistic accuracy measures.
A similar situation can arise with tasks for which inter-rater reliability is modest but deep learning models produce very high accuracies.
When coupled with imprudence, data that is confounded, biased, skewed, or of low quality will produce models of dubious performance and limited generalizability.

Using a test set more than once will lead to biased estimates of the generalization performance  [@arxiv:1811.12808; @doi:10.1162/089976698300017197].
Deep supervised learning models should be trained, tuned, and tested on non-overlapping datasets.
The data used for testing should be locked and only used one-time for evaluating the final model after all tuning steps are completed.
Also, many conventional metrics for classification (e.g. area under the receiver operating characteristic curve or AUROC) have limited utility in cases of extreme class imbalance [@pmid:25738806].
Model performance should be evaluated with a carefully-picked panel of relevant metrics that make minimal assumptions about the composition of the testing data [@doi:10.1021/acs.molpharmaceut.7b00578], with particular consideration given to metrics that are most directly applicable to the task at hand.

Extreme cases warrant testing the robustness of the model and metrics on simulated data for which the ground truth is known.
Said simulations can be used to verify the correctness of the model’s implementation as well.
