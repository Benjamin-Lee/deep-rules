[
  {
    "id": "UeE0s74F",
    "URL": "https://arxiv.org/abs/1509.09292",
    "number": "1509.09292",
    "title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints",
    "issued": {
      "date-parts": [
        [
          2015,
          11,
          4
        ]
      ]
    },
    "author": [
      {
        "given": "David",
        "family": "Duvenaud"
      },
      {
        "given": "Dougal",
        "family": "Maclaurin"
      },
      {
        "given": "Jorge",
        "family": "Aguilera-Iparraguirre"
      },
      {
        "given": "Rafael",
        "family": "Gómez-Bombarelli"
      },
      {
        "given": "Timothy",
        "family": "Hirzel"
      },
      {
        "given": "Alán",
        "family": "Aspuru-Guzik"
      },
      {
        "given": "Ryan P.",
        "family": "Adams"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks. ",
    "note": "license: http://creativecommons.org/licenses/by/4.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1509.09292"
  },
  {
    "id": "iAeJlSAZ",
    "URL": "https://arxiv.org/abs/1511.06348",
    "number": "1511.06348",
    "title": "How much data is needed to train a medical image deep learning system to achieve necessary high accuracy?",
    "issued": {
      "date-parts": [
        [
          2016,
          1,
          11
        ]
      ]
    },
    "author": [
      {
        "given": "Junghwan",
        "family": "Cho"
      },
      {
        "given": "Kyewook",
        "family": "Lee"
      },
      {
        "given": "Ellie",
        "family": "Shin"
      },
      {
        "given": "Garry",
        "family": "Choy"
      },
      {
        "given": "Synho",
        "family": "Do"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  The use of Convolutional Neural Networks (CNN) in natural image classification systems has produced very impressive results. Combined with the inherent nature of medical images that make them ideal for deep-learning, further application of such systems to medical image classification holds much promise. However, the usefulness and potential impact of such a system can be completely negated if it does not reach a target accuracy. In this paper, we present a study on determining the optimum size of the training data set necessary to achieve high classification accuracy with low variance in medical image classification systems. The CNN was applied to classify axial Computed Tomography (CT) images into six anatomical classes. We trained the CNN using six different sizes of training data set (5, 10, 20, 50, 100, and 200) and then tested the resulting system with a total of 6000 CT images. All images were acquired from the Massachusetts General Hospital (MGH) Picture Archiving and Communication System (PACS). Using this data, we employ the learning curve approach to predict classification accuracy at a given training sample size. Our research will present a general methodology for determining the training data set size necessary to achieve a certain target classification accuracy that can be easily applied to other problems within such systems. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1511.06348"
  },
  {
    "id": "hOeUlCvS",
    "URL": "https://arxiv.org/abs/1603.04467",
    "number": "1603.04467",
    "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
    "issued": {
      "date-parts": [
        [
          2016,
          3,
          17
        ]
      ]
    },
    "author": [
      {
        "given": "Martín",
        "family": "Abadi"
      },
      {
        "given": "Ashish",
        "family": "Agarwal"
      },
      {
        "given": "Paul",
        "family": "Barham"
      },
      {
        "given": "Eugene",
        "family": "Brevdo"
      },
      {
        "given": "Zhifeng",
        "family": "Chen"
      },
      {
        "given": "Craig",
        "family": "Citro"
      },
      {
        "given": "Greg S.",
        "family": "Corrado"
      },
      {
        "given": "Andy",
        "family": "Davis"
      },
      {
        "given": "Jeffrey",
        "family": "Dean"
      },
      {
        "given": "Matthieu",
        "family": "Devin"
      },
      {
        "given": "Sanjay",
        "family": "Ghemawat"
      },
      {
        "given": "Ian",
        "family": "Goodfellow"
      },
      {
        "given": "Andrew",
        "family": "Harp"
      },
      {
        "given": "Geoffrey",
        "family": "Irving"
      },
      {
        "given": "Michael",
        "family": "Isard"
      },
      {
        "given": "Yangqing",
        "family": "Jia"
      },
      {
        "given": "Rafal",
        "family": "Jozefowicz"
      },
      {
        "given": "Lukasz",
        "family": "Kaiser"
      },
      {
        "given": "Manjunath",
        "family": "Kudlur"
      },
      {
        "given": "Josh",
        "family": "Levenberg"
      },
      {
        "given": "Dan",
        "family": "Mane"
      },
      {
        "given": "Rajat",
        "family": "Monga"
      },
      {
        "given": "Sherry",
        "family": "Moore"
      },
      {
        "given": "Derek",
        "family": "Murray"
      },
      {
        "given": "Chris",
        "family": "Olah"
      },
      {
        "given": "Mike",
        "family": "Schuster"
      },
      {
        "given": "Jonathon",
        "family": "Shlens"
      },
      {
        "given": "Benoit",
        "family": "Steiner"
      },
      {
        "given": "Ilya",
        "family": "Sutskever"
      },
      {
        "given": "Kunal",
        "family": "Talwar"
      },
      {
        "given": "Paul",
        "family": "Tucker"
      },
      {
        "given": "Vincent",
        "family": "Vanhoucke"
      },
      {
        "given": "Vijay",
        "family": "Vasudevan"
      },
      {
        "given": "Fernanda",
        "family": "Viegas"
      },
      {
        "given": "Oriol",
        "family": "Vinyals"
      },
      {
        "given": "Pete",
        "family": "Warden"
      },
      {
        "given": "Martin",
        "family": "Wattenberg"
      },
      {
        "given": "Martin",
        "family": "Wicke"
      },
      {
        "given": "Yuan",
        "family": "Yu"
      },
      {
        "given": "Xiaoqiang",
        "family": "Zheng"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1603.04467"
  },
  {
    "id": "15N3fu3hC",
    "URL": "https://arxiv.org/abs/1605.07723v3",
    "number": "1605.07723v3",
    "version": "v3",
    "title": "Data Programming: Creating Large Training Sets, Quickly",
    "issued": {
      "date-parts": [
        [
          2016,
          5,
          25
        ]
      ]
    },
    "author": [
      {
        "literal": "Alexander Ratner"
      },
      {
        "literal": "Christopher De Sa"
      },
      {
        "literal": "Sen Wu"
      },
      {
        "literal": "Daniel Selsam"
      },
      {
        "literal": "Christopher Ré"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users express weak supervision strategies or domain heuristics as labeling functions, which are programs that label subsets of the data, but that are noisy and may conflict. We show that by explicitly representing this training set labeling process as a generative model, we can \"denoise\" the generated training set, and establish theoretically that we can recover the parameters of these generative models in a handful of settings. We then show how to modify a discriminative loss function to make it noise-aware, and demonstrate our method over a range of discriminative models including logistic regression and LSTMs. Experimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data programming would have led to a new winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a state-of-the-art LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way for non-experts to create machine learning models when training data is limited or unavailable.",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1605.07723v3"
  },
  {
    "id": "1HbRTExaU",
    "URL": "https://arxiv.org/abs/1610.05820",
    "number": "1610.05820",
    "title": "Membership Inference Attacks against Machine Learning Models",
    "issued": {
      "date-parts": [
        [
          2017,
          4,
          4
        ]
      ]
    },
    "author": [
      {
        "given": "Reza",
        "family": "Shokri"
      },
      {
        "given": "Marco",
        "family": "Stronati"
      },
      {
        "given": "Congzheng",
        "family": "Song"
      },
      {
        "given": "Vitaly",
        "family": "Shmatikov"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on.\n  We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1610.05820"
  },
  {
    "id": "Exfv0f4l",
    "URL": "https://arxiv.org/abs/1706.03762",
    "number": "1706.03762",
    "title": "Attention Is All You Need",
    "issued": {
      "date-parts": [
        [
          2017,
          12,
          7
        ]
      ]
    },
    "author": [
      {
        "given": "Ashish",
        "family": "Vaswani"
      },
      {
        "given": "Noam",
        "family": "Shazeer"
      },
      {
        "given": "Niki",
        "family": "Parmar"
      },
      {
        "given": "Jakob",
        "family": "Uszkoreit"
      },
      {
        "given": "Llion",
        "family": "Jones"
      },
      {
        "given": "Aidan N.",
        "family": "Gomez"
      },
      {
        "given": "Lukasz",
        "family": "Kaiser"
      },
      {
        "given": "Illia",
        "family": "Polosukhin"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1706.03762"
  },
  {
    "id": "1GcEYQ07X",
    "URL": "https://arxiv.org/abs/1709.06560",
    "number": "1709.06560",
    "title": "Deep Reinforcement Learning that Matters",
    "issued": {
      "date-parts": [
        [
          2019,
          1,
          31
        ]
      ]
    },
    "author": [
      {
        "given": "Peter",
        "family": "Henderson"
      },
      {
        "given": "Riashat",
        "family": "Islam"
      },
      {
        "given": "Philip",
        "family": "Bachman"
      },
      {
        "given": "Joelle",
        "family": "Pineau"
      },
      {
        "given": "Doina",
        "family": "Precup"
      },
      {
        "given": "David",
        "family": "Meger"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1709.06560"
  },
  {
    "id": "aqgi0yxG",
    "URL": "https://arxiv.org/abs/1803.01271",
    "number": "1803.01271",
    "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
    "issued": {
      "date-parts": [
        [
          2018,
          4,
          20
        ]
      ]
    },
    "author": [
      {
        "given": "Shaojie",
        "family": "Bai"
      },
      {
        "given": "J. Zico",
        "family": "Kolter"
      },
      {
        "given": "Vladlen",
        "family": "Koltun"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN . ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1803.01271"
  },
  {
    "id": "uBcf6TJ2",
    "URL": "https://arxiv.org/abs/1803.04765",
    "number": "1803.04765",
    "title": "Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning",
    "issued": {
      "date-parts": [
        [
          2018,
          3,
          14
        ]
      ]
    },
    "author": [
      {
        "given": "Nicolas",
        "family": "Papernot"
      },
      {
        "given": "Patrick",
        "family": "McDaniel"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model's training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1803.04765"
  },
  {
    "id": "2bsGpiQt",
    "URL": "https://arxiv.org/abs/1805.11783",
    "number": "1805.11783",
    "title": "To Trust Or Not To Trust A Classifier",
    "issued": {
      "date-parts": [
        [
          2018,
          10,
          30
        ]
      ]
    },
    "author": [
      {
        "given": "Heinrich",
        "family": "Jiang"
      },
      {
        "given": "Been",
        "family": "Kim"
      },
      {
        "given": "Melody Y.",
        "family": "Guan"
      },
      {
        "given": "Maya",
        "family": "Gupta"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Knowing when a classifier's prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier's predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier's discriminant or confidence score; however, we show there exists an alternative that is more effective in many situations. We propose a new score, called the trust score, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier's confidence score as well as many other baselines. Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1805.11783"
  },
  {
    "id": "ra8TSEHY",
    "URL": "https://arxiv.org/abs/1806.10282",
    "number": "1806.10282",
    "title": "Auto-Keras: An Efficient Neural Architecture Search System",
    "issued": {
      "date-parts": [
        [
          2019,
          3,
          27
        ]
      ]
    },
    "author": [
      {
        "given": "Haifeng",
        "family": "Jin"
      },
      {
        "given": "Qingquan",
        "family": "Song"
      },
      {
        "given": "Xia",
        "family": "Hu"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Intensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1806.10282"
  },
  {
    "id": "E2JcoiqW",
    "URL": "https://arxiv.org/abs/1809.00238",
    "number": "1809.00238",
    "title": "A Machine Learning Driven IoT Solution for Noise Classification in Smart Cities",
    "issued": {
      "date-parts": [
        [
          2018,
          9,
          5
        ]
      ]
    },
    "author": [
      {
        "given": "Yasser",
        "family": "Alsouda"
      },
      {
        "given": "Sabri",
        "family": "Pllana"
      },
      {
        "given": "Arianit",
        "family": "Kurti"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  We present a machine learning based method for noise classification using a low-power and inexpensive IoT unit. We use Mel-frequency cepstral coefficients for audio feature extraction and supervised classification algorithms (that is, support vector machine and k-nearest neighbors) for noise classification. We evaluate our approach experimentally with a dataset of about 3000 sound samples grouped in eight sound classes (such as, car horn, jackhammer, or street music). We explore the parameter space of support vector machine and k-nearest neighbors algorithms to estimate the optimal parameter values for classification of sound samples in the dataset under study. We achieve a noise classification accuracy in the range 85% -- 100%. Training and testing of our k-nearest neighbors (k = 1) implementation on Raspberry Pi Zero W is less than a second for a dataset with features of more than 3000 sound samples. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1809.00238"
  },
  {
    "id": "Tx4vUlOa",
    "URL": "https://arxiv.org/abs/1810.08055",
    "number": "1810.08055",
    "title": "Ten Simple Rules for Reproducible Research in Jupyter Notebooks",
    "issued": {
      "date-parts": [
        [
          2018,
          10,
          19
        ]
      ]
    },
    "author": [
      {
        "given": "Adam",
        "family": "Rule"
      },
      {
        "given": "Amanda",
        "family": "Birmingham"
      },
      {
        "given": "Cristal",
        "family": "Zuniga"
      },
      {
        "given": "Ilkay",
        "family": "Altintas"
      },
      {
        "given": "Shih-Cheng",
        "family": "Huang"
      },
      {
        "given": "Rob",
        "family": "Knight"
      },
      {
        "given": "Niema",
        "family": "Moshiri"
      },
      {
        "given": "Mai H.",
        "family": "Nguyen"
      },
      {
        "given": "Sara Brin",
        "family": "Rosenthal"
      },
      {
        "given": "Fernando",
        "family": "Pérez"
      },
      {
        "given": "Peter W.",
        "family": "Rose"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Reproducibility of computational studies is a hallmark of scientific methodology. It enables researchers to build with confidence on the methods and findings of others, reuse and extend computational pipelines, and thereby drive scientific progress. Since many experimental studies rely on computational analyses, biologists need guidance on how to set up and document reproducible data analyses or simulations.\n  In this paper, we address several questions about reproducibility. For example, what are the technical and non-technical barriers to reproducible computational studies? What opportunities and challenges do computational notebooks offer to overcome some of these barriers? What tools are available and how can they be used effectively?\n  We have developed a set of rules to serve as a guide to scientists with a specific focus on computational notebook systems, such as Jupyter Notebooks, which have become a tool of choice for many applications. Notebooks combine detailed workflows with narrative text and visualization of results. Combined with software repositories and open source licensing, notebooks are powerful tools for transparent, collaborative, reproducible, and reusable data analyses. ",
    "note": "license: http://creativecommons.org/licenses/by/4.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1810.08055"
  },
  {
    "id": "3326vtLW",
    "URL": "https://arxiv.org/abs/1811.00778",
    "number": "1811.00778",
    "title": "Towards the AlexNet Moment for Homomorphic Encryption: HCNN, theFirst Homomorphic CNN on Encrypted Data with GPUs",
    "issued": {
      "date-parts": [
        [
          2020,
          8,
          20
        ]
      ]
    },
    "author": [
      {
        "given": "Ahmad Al",
        "family": "Badawi"
      },
      {
        "given": "Jin",
        "family": "Chao"
      },
      {
        "given": "Jie",
        "family": "Lin"
      },
      {
        "given": "Chan Fook",
        "family": "Mun"
      },
      {
        "given": "Jun Jie",
        "family": "Sim"
      },
      {
        "given": "Benjamin Hong Meng",
        "family": "Tan"
      },
      {
        "given": "Xiao",
        "family": "Nan"
      },
      {
        "given": "Khin Mi Mi",
        "family": "Aung"
      },
      {
        "given": "Vijay Ramaseshan",
        "family": "Chandrasekhar"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Deep Learning as a Service (DLaaS) stands as a promising solution for cloud-based inference applications. In this setting, the cloud has a pre-learned model whereas the user has samples on which she wants to run the model. The biggest concern with DLaaS is user privacy if the input samples are sensitive data. We provide here an efficient privacy-preserving system by employing high-end technologies such as Fully Homomorphic Encryption (FHE), Convolutional Neural Networks (CNNs) and Graphics Processing Units (GPUs). FHE, with its widely-known feature of computing on encrypted data, empowers a wide range of privacy-concerned applications. This comes at high cost as it requires enormous computing power. In this paper, we show how to accelerate the performance of running CNNs on encrypted data with GPUs. We evaluated two CNNs to classify homomorphically the MNIST and CIFAR-10 datasets. Our solution achieved a sufficient security level (> 80 bit) and reasonable classification accuracy (99%) and (77.55%) for MNIST and CIFAR-10, respectively. In terms of latency, we could classify an image in 5.16 seconds and 304.43 seconds for MNIST and CIFAR-10, respectively. Our system can also classify a batch of images (> 8,000) without extra overhead. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1811.00778"
  },
  {
    "id": "1HuQe3Z8X",
    "URL": "https://arxiv.org/abs/1811.04017",
    "number": "1811.04017",
    "title": "A generic framework for privacy preserving deep learning",
    "issued": {
      "date-parts": [
        [
          2018,
          11,
          14
        ]
      ]
    },
    "author": [
      {
        "given": "Theo",
        "family": "Ryffel"
      },
      {
        "given": "Andrew",
        "family": "Trask"
      },
      {
        "given": "Morten",
        "family": "Dahl"
      },
      {
        "given": "Bobby",
        "family": "Wagner"
      },
      {
        "given": "Jason",
        "family": "Mancuso"
      },
      {
        "given": "Daniel",
        "family": "Rueckert"
      },
      {
        "given": "Jonathan",
        "family": "Passerat-Palmbach"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  We detail a new framework for privacy preserving deep learning and discuss its assets. The framework puts a premium on ownership and secure processing of data and introduces a valuable representation based on chains of commands and tensors. This abstraction allows one to implement complex privacy preserving constructs such as Federated Learning, Secure Multiparty Computation, and Differential Privacy while still exposing a familiar deep learning API to the end-user. We report early results on the Boston Housing and Pima Indian Diabetes datasets. While the privacy features apart from Differential Privacy do not impact the prediction accuracy, the current implementation of the framework introduces a significant overhead in performance, which will be addressed at a later stage of the development. We believe this work is an important milestone introducing the first reliable, general framework for privacy preserving deep learning. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1811.04017"
  },
  {
    "id": "1CDx6NYSj",
    "URL": "https://arxiv.org/abs/1811.12808",
    "number": "1811.12808",
    "title": "Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning",
    "issued": {
      "date-parts": [
        [
          2020,
          11,
          12
        ]
      ]
    },
    "author": [
      {
        "given": "Sebastian",
        "family": "Raschka"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one-out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1811.12808"
  },
  {
    "id": "eJgWbXRz",
    "URL": "https://arxiv.org/abs/1812.01484",
    "number": "1812.01484",
    "title": "Privacy-Preserving Distributed Deep Learning for Clinical Data",
    "issued": {
      "date-parts": [
        [
          2018,
          12,
          5
        ]
      ]
    },
    "author": [
      {
        "given": "Brett K.",
        "family": "Beaulieu-Jones"
      },
      {
        "given": "William",
        "family": "Yuan"
      },
      {
        "given": "Samuel G.",
        "family": "Finlayson"
      },
      {
        "given": "Zhiwei Steven",
        "family": "Wu"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Deep learning with medical data often requires larger samples sizes than are available at single providers. While data sharing among institutions is desirable to train more accurate and sophisticated models, it can lead to severe privacy concerns due the sensitive nature of the data. This problem has motivated a number of studies on distributed training of neural networks that do not require direct sharing of the training data. However, simple distributed training does not offer provable privacy guarantees to satisfy technical safe standards and may reveal information about the underlying patients. We present a method to train neural networks for clinical data in a distributed fashion under differential privacy. We demonstrate these methods on two datasets that include information from multiple independent sites, the eICU collaborative Research Database and The Cancer Genome Atlas. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1812.01484"
  },
  {
    "id": "sqqjVz8I",
    "URL": "https://arxiv.org/abs/1906.01998",
    "number": "1906.01998",
    "title": "The Secrets of Machine Learning: Ten Things You Wish You Had Known Earlier to be More Effective at Data Analysis",
    "issued": {
      "date-parts": [
        [
          2019,
          6,
          6
        ]
      ]
    },
    "author": [
      {
        "given": "Cynthia",
        "family": "Rudin"
      },
      {
        "given": "David",
        "family": "Carlson"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Despite the widespread usage of machine learning throughout organizations, there are some key principles that are commonly missed. In particular: 1) There are at least four main families for supervised learning: logical modeling methods, linear combination methods, case-based reasoning methods, and iterative summarization methods. 2) For many application domains, almost all machine learning methods perform similarly (with some caveats). Deep learning methods, which are the leading technique for computer vision problems, do not maintain an edge over other methods for most problems (and there are reasons why). 3) Neural networks are hard to train and weird stuff often happens when you try to train them. 4) If you don't use an interpretable model, you can make bad mistakes. 5) Explanations can be misleading and you can't trust them. 6) You can pretty much always find an accurate-yet-interpretable model, even for deep neural networks. 7) Special properties such as decision making or robustness must be built in, they don't happen on their own. 8) Causal inference is different than prediction (correlation is not causation). 9) There is a method to the madness of deep neural architectures, but not always. 10) It is a myth that artificial intelligence can do anything. ",
    "note": "license: http://creativecommons.org/licenses/by/4.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1906.01998"
  },
  {
    "id": "1CnZlKOVj",
    "URL": "https://arxiv.org/abs/1906.02243",
    "number": "1906.02243",
    "title": "Energy and Policy Considerations for Deep Learning in NLP",
    "issued": {
      "date-parts": [
        [
          2019,
          6,
          7
        ]
      ]
    },
    "author": [
      {
        "given": "Emma",
        "family": "Strubell"
      },
      {
        "given": "Ananya",
        "family": "Ganesh"
      },
      {
        "given": "Andrew",
        "family": "McCallum"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1906.02243"
  },
  {
    "id": "iTP4h1rX",
    "URL": "https://arxiv.org/abs/1912.01703",
    "number": "1912.01703",
    "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
    "issued": {
      "date-parts": [
        [
          2019,
          12,
          5
        ]
      ]
    },
    "author": [
      {
        "given": "Adam",
        "family": "Paszke"
      },
      {
        "given": "Sam",
        "family": "Gross"
      },
      {
        "given": "Francisco",
        "family": "Massa"
      },
      {
        "given": "Adam",
        "family": "Lerer"
      },
      {
        "given": "James",
        "family": "Bradbury"
      },
      {
        "given": "Gregory",
        "family": "Chanan"
      },
      {
        "given": "Trevor",
        "family": "Killeen"
      },
      {
        "given": "Zeming",
        "family": "Lin"
      },
      {
        "given": "Natalia",
        "family": "Gimelshein"
      },
      {
        "given": "Luca",
        "family": "Antiga"
      },
      {
        "given": "Alban",
        "family": "Desmaison"
      },
      {
        "given": "Andreas",
        "family": "Köpf"
      },
      {
        "given": "Edward",
        "family": "Yang"
      },
      {
        "given": "Zach",
        "family": "DeVito"
      },
      {
        "given": "Martin",
        "family": "Raison"
      },
      {
        "given": "Alykhan",
        "family": "Tejani"
      },
      {
        "given": "Sasank",
        "family": "Chilamkurthy"
      },
      {
        "given": "Benoit",
        "family": "Steiner"
      },
      {
        "given": "Lu",
        "family": "Fang"
      },
      {
        "given": "Junjie",
        "family": "Bai"
      },
      {
        "given": "Soumith",
        "family": "Chintala"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.\n  In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.\n  We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:1912.01703"
  },
  {
    "id": "cRG2FGOV",
    "URL": "https://arxiv.org/abs/2001.02522",
    "number": "2001.02522",
    "title": "On Interpretability of Artificial Neural Networks: A Survey",
    "issued": {
      "date-parts": [
        [
          2021,
          1,
          26
        ]
      ]
    },
    "author": [
      {
        "given": "Fenglei",
        "family": "Fan"
      },
      {
        "given": "Jinjun",
        "family": "Xiong"
      },
      {
        "given": "Mengzhou",
        "family": "Li"
      },
      {
        "given": "Ge",
        "family": "Wang"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Deep learning as represented by the artificial deep neural networks (DNNs) has achieved great success in many important areas that deal with text, images, videos, graphs, and so on. However, the black-box nature of DNNs has become one of the primary obstacles for their wide acceptance in mission-critical applications such as medical diagnosis and therapy. Due to the huge potential of deep learning, interpreting neural networks has recently attracted much research attention. In this paper, based on our comprehensive taxonomy, we systematically review recent studies in understanding the mechanism of neural networks, describe applications of interpretability especially in medicine, and discuss future directions of interpretability research, such as in relation to fuzzy logic and brain science. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:2001.02522"
  },
  {
    "id": "703iLzmh",
    "URL": "https://arxiv.org/abs/2004.03497",
    "number": "2004.03497",
    "title": "ProGen: Language Modeling for Protein Generation",
    "issued": {
      "date-parts": [
        [
          2020,
          4,
          8
        ]
      ]
    },
    "author": [
      {
        "given": "Ali",
        "family": "Madani"
      },
      {
        "given": "Bryan",
        "family": "McCann"
      },
      {
        "given": "Nikhil",
        "family": "Naik"
      },
      {
        "given": "Nitish Shirish",
        "family": "Keskar"
      },
      {
        "given": "Namrata",
        "family": "Anand"
      },
      {
        "given": "Raphael R.",
        "family": "Eguchi"
      },
      {
        "given": "Po-Ssu",
        "family": "Huang"
      },
      {
        "given": "Richard",
        "family": "Socher"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on ~280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:2004.03497"
  },
  {
    "id": "bYOaJHMe",
    "URL": "https://arxiv.org/abs/2005.14165",
    "number": "2005.14165",
    "title": "Language Models are Few-Shot Learners",
    "issued": {
      "date-parts": [
        [
          2020,
          7,
          24
        ]
      ]
    },
    "author": [
      {
        "given": "Tom B.",
        "family": "Brown"
      },
      {
        "given": "Benjamin",
        "family": "Mann"
      },
      {
        "given": "Nick",
        "family": "Ryder"
      },
      {
        "given": "Melanie",
        "family": "Subbiah"
      },
      {
        "given": "Jared",
        "family": "Kaplan"
      },
      {
        "given": "Prafulla",
        "family": "Dhariwal"
      },
      {
        "given": "Arvind",
        "family": "Neelakantan"
      },
      {
        "given": "Pranav",
        "family": "Shyam"
      },
      {
        "given": "Girish",
        "family": "Sastry"
      },
      {
        "given": "Amanda",
        "family": "Askell"
      },
      {
        "given": "Sandhini",
        "family": "Agarwal"
      },
      {
        "given": "Ariel",
        "family": "Herbert-Voss"
      },
      {
        "given": "Gretchen",
        "family": "Krueger"
      },
      {
        "given": "Tom",
        "family": "Henighan"
      },
      {
        "given": "Rewon",
        "family": "Child"
      },
      {
        "given": "Aditya",
        "family": "Ramesh"
      },
      {
        "given": "Daniel M.",
        "family": "Ziegler"
      },
      {
        "given": "Jeffrey",
        "family": "Wu"
      },
      {
        "given": "Clemens",
        "family": "Winter"
      },
      {
        "given": "Christopher",
        "family": "Hesse"
      },
      {
        "given": "Mark",
        "family": "Chen"
      },
      {
        "given": "Eric",
        "family": "Sigler"
      },
      {
        "given": "Mateusz",
        "family": "Litwin"
      },
      {
        "given": "Scott",
        "family": "Gray"
      },
      {
        "given": "Benjamin",
        "family": "Chess"
      },
      {
        "given": "Jack",
        "family": "Clark"
      },
      {
        "given": "Christopher",
        "family": "Berner"
      },
      {
        "given": "Sam",
        "family": "McCandlish"
      },
      {
        "given": "Alec",
        "family": "Radford"
      },
      {
        "given": "Ilya",
        "family": "Sutskever"
      },
      {
        "given": "Dario",
        "family": "Amodei"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:2005.14165"
  },
  {
    "id": "19sSkUYfR",
    "URL": "https://arxiv.org/abs/2007.06225",
    "number": "2007.06225",
    "title": "ProtTrans: Towards Cracking the Language of Life's Code Through Self-Supervised Deep Learning and High Performance Computing",
    "issued": {
      "date-parts": [
        [
          2020,
          7,
          22
        ]
      ]
    },
    "author": [
      {
        "given": "Ahmed",
        "family": "Elnaggar"
      },
      {
        "given": "Michael",
        "family": "Heinzinger"
      },
      {
        "given": "Christian",
        "family": "Dallago"
      },
      {
        "given": "Ghalia",
        "family": "Rihawi"
      },
      {
        "given": "Yu",
        "family": "Wang"
      },
      {
        "given": "Llion",
        "family": "Jones"
      },
      {
        "given": "Tom",
        "family": "Gibbs"
      },
      {
        "given": "Tamas",
        "family": "Feher"
      },
      {
        "given": "Christoph",
        "family": "Angerer"
      },
      {
        "given": "Martin",
        "family": "Steinegger"
      },
      {
        "given": "Debsindhu",
        "family": "Bhowmik"
      },
      {
        "given": "Burkhard",
        "family": "Rost"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models (LMs) taken from Natural Language Processing (NLP). These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive language models (Transformer-XL, XLNet) and two auto-encoder models (Bert, Albert) on data from UniRef and BFD containing up to 393 billion amino acids (words) from 2.1 billion protein sequences (22- and 112-times the entire English Wikipedia). The LMs were trained on the Summit supercomputer at Oak Ridge National Laboratory (ORNL), using 936 nodes (total 5616 GPUs) and one TPU Pod (V3-512 or V3-1024). We validated the advantage of up-scaling LMs to larger models supported by bigger data by predicting secondary structure (3-states: Q3=76-84, 8-states: Q8=65-73), sub-cellular localization for 10 cellular compartments (Q10=74) and whether a protein is membrane-bound or water-soluble (Q2=89). Dimensionality reduction revealed that the LM-embeddings from unlabeled data (only protein sequences) captured important biophysical properties governing protein shape. This implied learning some of the grammar of the language of life realized in protein sequences. The successful up-scaling of protein LMs through HPC to larger data sets slightly reduced the gap between models trained on evolutionary information and LMs.\n  The official GitHub repository: https://github.com/agemagician/ProtTrans ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:2007.06225"
  },
  {
    "id": "N3a59ayQ",
    "URL": "https://arxiv.org/abs/2007.15779",
    "number": "2007.15779",
    "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
    "issued": {
      "date-parts": [
        [
          2021,
          2,
          15
        ]
      ]
    },
    "author": [
      {
        "given": "Yu",
        "family": "Gu"
      },
      {
        "given": "Robert",
        "family": "Tinn"
      },
      {
        "given": "Hao",
        "family": "Cheng"
      },
      {
        "given": "Michael",
        "family": "Lucas"
      },
      {
        "given": "Naoto",
        "family": "Usuyama"
      },
      {
        "given": "Xiaodong",
        "family": "Liu"
      },
      {
        "given": "Tristan",
        "family": "Naumann"
      },
      {
        "given": "Jianfeng",
        "family": "Gao"
      },
      {
        "given": "Hoifung",
        "family": "Poon"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this paper, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly-available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition (NER). To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding & Reasoning Benchmark) at https://aka.ms/BLURB. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:2007.15779"
  },
  {
    "id": "EnNKKBjl",
    "URL": "https://arxiv.org/abs/2010.09885",
    "number": "2010.09885",
    "title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
    "issued": {
      "date-parts": [
        [
          2020,
          10,
          26
        ]
      ]
    },
    "author": [
      {
        "given": "Seyone",
        "family": "Chithrananda"
      },
      {
        "given": "Gabriel",
        "family": "Grand"
      },
      {
        "given": "Bharath",
        "family": "Ramsundar"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  GNNs and chemical fingerprints are the predominant approaches to representing molecules for property prediction. However, in NLP, transformers have become the de-facto standard for representation learning thanks to their strong downstream task transfer. In parallel, the software ecosystem around transformers is maturing rapidly, with libraries like HuggingFace and BertViz enabling streamlined training and introspection. In this work, we make one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via our ChemBERTa model. ChemBERTa scales well with pretraining dataset size, offering competitive downstream performance on MoleculeNet and useful attention-based visualization modalities. Our results suggest that transformers offer a promising avenue of future work for molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77M SMILES from PubChem suitable for large-scale self-supervised pretraining. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: arxiv:2010.09885"
  },
  {
    "type": "article-journal",
    "id": "f6P8XTkP",
    "author": [
      {
        "family": "Ho",
        "given": "Anita"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019,
          1
        ]
      ]
    },
    "container-title": "Hastings Center Report",
    "DOI": "10.1002/hast.977",
    "volume": "49",
    "issue": "1",
    "page": "36-39",
    "publisher": "Wiley",
    "title": "Deep Ethical Learning: Taking the Interplay of Human and Artificial Intelligence Seriously",
    "URL": "https://doi.org/ggsqtt",
    "PMID": "30790317",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1002/hast.977"
  },
  {
    "type": "chapter",
    "id": "aYxTroNH",
    "author": [
      {
        "family": "Raschka",
        "given": "Sebastian"
      },
      {
        "family": "Scott",
        "given": "Anne M."
      },
      {
        "family": "Huertas",
        "given": "Mar"
      },
      {
        "family": "Li",
        "given": "Weiming"
      },
      {
        "family": "Kuhn",
        "given": "Leslie A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "container-title": "Methods in Molecular Biology",
    "DOI": "10.1007/978-1-4939-7756-7_16",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Automated Inference of Chemical Discriminants of Biological Activity",
    "URL": "https://doi.org/ghk2pg",
    "PMID": "29594779",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1007/978-1-4939-7756-7_16"
  },
  {
    "type": "chapter",
    "id": "12tC5JTV6",
    "author": [
      {
        "family": "Olson",
        "given": "Randal S."
      },
      {
        "family": "Urbanowicz",
        "given": "Ryan J."
      },
      {
        "family": "Andrews",
        "given": "Peter C."
      },
      {
        "family": "Lavender",
        "given": "Nicole A."
      },
      {
        "family": "Kidd",
        "given": "La Creis"
      },
      {
        "family": "Moore",
        "given": "Jason H."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016
        ]
      ]
    },
    "container-title": "Lecture Notes in Computer Science",
    "DOI": "10.1007/978-3-319-31204-0_9",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Automating Biomedical Data Science Through Tree-Based Pipeline Optimization",
    "URL": "https://doi.org/ggfptv",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1007/978-3-319-31204-0_9"
  },
  {
    "type": "book",
    "id": "JT3rHKc7",
    "issued": {
      "date-parts": [
        [
          2012
        ]
      ]
    },
    "container-title": "Lecture Notes in Computer Science",
    "DOI": "10.1007/978-3-642-35289-8",
    "volume": "7700",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Neural Networks: Tricks of the Trade",
    "URL": "https://doi.org/gfvtvt",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1007/978-3-642-35289-8"
  },
  {
    "type": "chapter",
    "id": "h7MUsYb3",
    "author": [
      {
        "family": "Mathew",
        "given": "Amitha"
      },
      {
        "family": "Amudha",
        "given": "P."
      },
      {
        "family": "Sivakumari",
        "given": "S."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2021
        ]
      ]
    },
    "container-title": "Advances in Intelligent Systems and Computing",
    "DOI": "10.1007/978-981-15-3383-9_54",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Deep Learning Techniques: An Overview",
    "URL": "https://doi.org/ghjtg6",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1007/978-981-15-3383-9_54"
  },
  {
    "type": "article-journal",
    "id": "xwsS0Nlg",
    "author": [
      {
        "family": "Cybenko",
        "given": "G."
      }
    ],
    "issued": {
      "date-parts": [
        [
          1989,
          12
        ]
      ]
    },
    "container-title": "Mathematics of Control, Signals, and Systems",
    "DOI": "10.1007/bf02551274",
    "volume": "2",
    "issue": "4",
    "page": "303-314",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Approximation by superpositions of a sigmoidal function",
    "URL": "https://doi.org/dp3968",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1007/bf02551274"
  },
  {
    "type": "article-journal",
    "id": "x6HXFAS4",
    "author": [
      {
        "family": "Rajkomar",
        "given": "Alvin"
      },
      {
        "family": "Lingam",
        "given": "Sneha"
      },
      {
        "family": "Taylor",
        "given": "Andrew G."
      },
      {
        "family": "Blum",
        "given": "Michael"
      },
      {
        "family": "Mongan",
        "given": "John"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016,
          10,
          11
        ]
      ]
    },
    "container-title": "Journal of Digital Imaging",
    "DOI": "10.1007/s10278-016-9914-9",
    "volume": "30",
    "issue": "1",
    "page": "95-101",
    "publisher": "Springer Science and Business Media LLC",
    "title": "High-Throughput Classification of Radiographs Using Deep Convolutional Neural Networks",
    "URL": "https://doi.org/gcgk7v",
    "PMCID": "PMC5267603",
    "PMID": "27730417",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1007/s10278-016-9914-9"
  },
  {
    "type": "article-journal",
    "id": "cBVeXnZx",
    "author": [
      {
        "family": "Russakovsky",
        "given": "Olga"
      },
      {
        "family": "Deng",
        "given": "Jia"
      },
      {
        "family": "Su",
        "given": "Hao"
      },
      {
        "family": "Krause",
        "given": "Jonathan"
      },
      {
        "family": "Satheesh",
        "given": "Sanjeev"
      },
      {
        "family": "Ma",
        "given": "Sean"
      },
      {
        "family": "Huang",
        "given": "Zhiheng"
      },
      {
        "family": "Karpathy",
        "given": "Andrej"
      },
      {
        "family": "Khosla",
        "given": "Aditya"
      },
      {
        "family": "Bernstein",
        "given": "Michael"
      },
      {
        "family": "Berg",
        "given": "Alexander C."
      },
      {
        "family": "Fei-Fei",
        "given": "Li"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2015,
          4,
          11
        ]
      ]
    },
    "container-title": "International Journal of Computer Vision",
    "DOI": "10.1007/s11263-015-0816-y",
    "volume": "115",
    "issue": "3",
    "page": "211-252",
    "publisher": "Springer Science and Business Media LLC",
    "title": "ImageNet Large Scale Visual Recognition Challenge",
    "URL": "https://doi.org/gcgk7w",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1007/s11263-015-0816-y"
  },
  {
    "type": "article-journal",
    "id": "sUd4ks2q",
    "author": [
      {
        "family": "Yasaka",
        "given": "Koichiro"
      },
      {
        "family": "Akai",
        "given": "Hiroyuki"
      },
      {
        "family": "Kunimatsu",
        "given": "Akira"
      },
      {
        "family": "Kiryu",
        "given": "Shigeru"
      },
      {
        "family": "Abe",
        "given": "Osamu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          3,
          1
        ]
      ]
    },
    "container-title": "Japanese Journal of Radiology",
    "DOI": "10.1007/s11604-018-0726-3",
    "volume": "36",
    "issue": "4",
    "page": "257-272",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Deep learning with convolutional neural network in radiology",
    "URL": "https://doi.org/ggb3tf",
    "PMID": "29498017",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1007/s11604-018-0726-3"
  },
  {
    "type": "article-journal",
    "id": "1BnILgle7",
    "author": [
      {
        "family": "Hornik",
        "given": "Kurt"
      }
    ],
    "issued": {
      "date-parts": [
        [
          1991
        ]
      ]
    },
    "container-title": "Neural Networks",
    "DOI": "10.1016/0893-6080(91)90009-t",
    "volume": "4",
    "issue": "2",
    "page": "251-257",
    "publisher": "Elsevier BV",
    "title": "Approximation capabilities of multilayer feedforward networks",
    "URL": "https://doi.org/dzwxkd",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1016/0893-6080(91)90009-t"
  },
  {
    "type": "article-journal",
    "id": "zGSQSBXa",
    "author": [
      {
        "family": "Zheng",
        "given": "Xin"
      },
      {
        "family": "Wang",
        "given": "Yong"
      },
      {
        "family": "Wang",
        "given": "Guoyou"
      },
      {
        "family": "Liu",
        "given": "Jianguo"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          4
        ]
      ]
    },
    "container-title": "Micron",
    "DOI": "10.1016/j.micron.2018.01.010",
    "volume": "107",
    "page": "55-71",
    "publisher": "Elsevier BV",
    "title": "Fast and robust segmentation of white blood cell images by self-supervised learning",
    "URL": "https://doi.org/gdfh65",
    "PMID": "29425969",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1016/j.micron.2018.01.010"
  },
  {
    "type": "article-journal",
    "id": "d0Mdu670",
    "author": [
      {
        "family": "Raschka",
        "given": "Sebastian"
      },
      {
        "family": "Kaufman",
        "given": "Benjamin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          8
        ]
      ]
    },
    "container-title": "Methods",
    "DOI": "10.1016/j.ymeth.2020.06.016",
    "volume": "180",
    "page": "89-110",
    "publisher": "Elsevier BV",
    "title": "Machine learning and AI-based approaches for bioactive ligand discovery and GPCR-ligand recognition",
    "URL": "https://doi.org/ghk2mf",
    "PMID": "32645448",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1016/j.ymeth.2020.06.016"
  },
  {
    "type": "article-journal",
    "id": "AE3ehMCc",
    "author": [
      {
        "family": "Leshno",
        "given": "Moshe"
      },
      {
        "family": "Lin",
        "given": "Vladimir Ya."
      },
      {
        "family": "Pinkus",
        "given": "Allan"
      },
      {
        "family": "Schocken",
        "given": "Shimon"
      }
    ],
    "issued": {
      "date-parts": [
        [
          1993,
          1
        ]
      ]
    },
    "container-title": "Neural Networks",
    "DOI": "10.1016/s0893-6080(05)80131-5",
    "volume": "6",
    "issue": "6",
    "page": "861-867",
    "publisher": "Elsevier BV",
    "title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function",
    "URL": "https://doi.org/bjjdg2",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1016/s0893-6080(05)80131-5"
  },
  {
    "type": "article-journal",
    "id": "980FAm5x",
    "author": [
      {
        "family": "Cooper",
        "given": "Gregory F."
      },
      {
        "family": "Aliferis",
        "given": "Constantin F."
      },
      {
        "family": "Ambrosino",
        "given": "Richard"
      },
      {
        "family": "Aronis",
        "given": "John"
      },
      {
        "family": "Buchanan",
        "given": "Bruce G."
      },
      {
        "family": "Caruana",
        "given": "Richard"
      },
      {
        "family": "Fine",
        "given": "Michael J."
      },
      {
        "family": "Glymour",
        "given": "Clark"
      },
      {
        "family": "Gordon",
        "given": "Geoffrey"
      },
      {
        "family": "Hanusa",
        "given": "Barbara H."
      },
      {
        "family": "Janosky",
        "given": "Janine E."
      },
      {
        "family": "Meek",
        "given": "Christopher"
      },
      {
        "family": "Mitchell",
        "given": "Tom"
      },
      {
        "family": "Richardson",
        "given": "Thomas"
      },
      {
        "family": "Spirtes",
        "given": "Peter"
      }
    ],
    "issued": {
      "date-parts": [
        [
          1997,
          2
        ]
      ]
    },
    "container-title": "Artificial Intelligence in Medicine",
    "DOI": "10.1016/s0933-3657(96)00367-3",
    "volume": "9",
    "issue": "2",
    "page": "107-138",
    "publisher": "Elsevier BV",
    "title": "An evaluation of machine-learning methods for predicting pneumonia mortality",
    "URL": "https://doi.org/b6vnmd",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1016/s0933-3657(96)00367-3"
  },
  {
    "type": "article-journal",
    "id": "rKXyJKNt",
    "author": [
      {
        "family": "Korotcov",
        "given": "Alexandru"
      },
      {
        "family": "Tkachenko",
        "given": "Valery"
      },
      {
        "family": "Russo",
        "given": "Daniel P."
      },
      {
        "family": "Ekins",
        "given": "Sean"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          11,
          13
        ]
      ]
    },
    "container-title": "Molecular Pharmaceutics",
    "DOI": "10.1021/acs.molpharmaceut.7b00578",
    "volume": "14",
    "issue": "12",
    "page": "4462-4475",
    "publisher": "American Chemical Society (ACS)",
    "title": "Comparison of Deep Learning With Multiple Machine Learning Methods and Metrics Using Diverse Drug Discovery Data Sets",
    "URL": "https://doi.org/gcj4p2",
    "PMCID": "PMC5741413",
    "PMID": "29096442",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1021/acs.molpharmaceut.7b00578"
  },
  {
    "type": "article-journal",
    "id": "yqAEYaMg",
    "author": [
      {
        "family": "Chuang",
        "given": "Kangway V."
      },
      {
        "family": "Keiser",
        "given": "Michael J."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          10,
          19
        ]
      ]
    },
    "container-title": "ACS Chemical Biology",
    "DOI": "10.1021/acschembio.8b00881",
    "volume": "13",
    "issue": "10",
    "page": "2819-2821",
    "publisher": "American Chemical Society (ACS)",
    "title": "Adversarial Controls for Scientific Machine Learning",
    "URL": "https://doi.org/gfk9mh",
    "PMID": "30336670",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1021/acschembio.8b00881"
  },
  {
    "type": "article-journal",
    "id": "BeijBSRE",
    "author": [
      {
        "family": "LeCun",
        "given": "Yann"
      },
      {
        "family": "Bengio",
        "given": "Yoshua"
      },
      {
        "family": "Hinton",
        "given": "Geoffrey"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2015,
          5,
          27
        ]
      ]
    },
    "container-title": "Nature",
    "DOI": "10.1038/nature14539",
    "volume": "521",
    "issue": "7553",
    "page": "436-444",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Deep learning",
    "URL": "https://doi.org/bmqp",
    "PMID": "26017442",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1038/nature14539"
  },
  {
    "type": "article-journal",
    "id": "Qh7xTLwz",
    "author": [
      {
        "family": "Beaulieu-Jones",
        "given": "Brett K"
      },
      {
        "family": "Greene",
        "given": "Casey S"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          3,
          13
        ]
      ]
    },
    "container-title": "Nature Biotechnology",
    "DOI": "10.1038/nbt.3780",
    "volume": "35",
    "issue": "4",
    "page": "342-346",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Reproducibility of computational workflows is automated using continuous analysis",
    "URL": "https://doi.org/f9ttx6",
    "PMCID": "PMC6103790",
    "PMID": "28288103",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1038/nbt.3780"
  },
  {
    "type": "article-journal",
    "id": "YuxbleXb",
    "author": [
      {
        "family": "Brazma",
        "given": "Alvis"
      },
      {
        "family": "Hingamp",
        "given": "Pascal"
      },
      {
        "family": "Quackenbush",
        "given": "John"
      },
      {
        "family": "Sherlock",
        "given": "Gavin"
      },
      {
        "family": "Spellman",
        "given": "Paul"
      },
      {
        "family": "Stoeckert",
        "given": "Chris"
      },
      {
        "family": "Aach",
        "given": "John"
      },
      {
        "family": "Ansorge",
        "given": "Wilhelm"
      },
      {
        "family": "Ball",
        "given": "Catherine A."
      },
      {
        "family": "Causton",
        "given": "Helen C."
      },
      {
        "family": "Gaasterland",
        "given": "Terry"
      },
      {
        "family": "Glenisson",
        "given": "Patrick"
      },
      {
        "family": "Holstege",
        "given": "Frank C.P."
      },
      {
        "family": "Kim",
        "given": "Irene F."
      },
      {
        "family": "Markowitz",
        "given": "Victor"
      },
      {
        "family": "Matese",
        "given": "John C."
      },
      {
        "family": "Parkinson",
        "given": "Helen"
      },
      {
        "family": "Robinson",
        "given": "Alan"
      },
      {
        "family": "Sarkans",
        "given": "Ugis"
      },
      {
        "family": "Schulze-Kremer",
        "given": "Steffen"
      },
      {
        "family": "Stewart",
        "given": "Jason"
      },
      {
        "family": "Taylor",
        "given": "Ronald"
      },
      {
        "family": "Vilo",
        "given": "Jaak"
      },
      {
        "family": "Vingron",
        "given": "Martin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2001,
          12
        ]
      ]
    },
    "container-title": "Nature Genetics",
    "DOI": "10.1038/ng1201-365",
    "volume": "29",
    "issue": "4",
    "page": "365-371",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Minimum information about a microarray experiment (MIAME)—toward standards for microarray data",
    "URL": "https://doi.org/ck257n",
    "PMID": "11726920",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1038/ng1201-365"
  },
  {
    "type": "article-journal",
    "id": "esvPpSAp",
    "author": [
      {
        "family": "Vision",
        "given": "Todd"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2010,
          6,
          30
        ]
      ]
    },
    "container-title": "Nature Precedings",
    "DOI": "10.1038/npre.2010.4595.1",
    "publisher": "Springer Science and Business Media LLC",
    "title": "The Dryad Digital Repository: Published evolutionary data as part of the greater data ecosystem",
    "URL": "https://doi.org/ghk2km",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1038/npre.2010.4595.1"
  },
  {
    "type": "article-journal",
    "id": "mPnIAH38",
    "author": [
      {
        "family": "Leek",
        "given": "Jeffrey T."
      },
      {
        "family": "Scharpf",
        "given": "Robert B."
      },
      {
        "family": "Bravo",
        "given": "Héctor Corrada"
      },
      {
        "family": "Simcha",
        "given": "David"
      },
      {
        "family": "Langmead",
        "given": "Benjamin"
      },
      {
        "family": "Johnson",
        "given": "W. Evan"
      },
      {
        "family": "Geman",
        "given": "Donald"
      },
      {
        "family": "Baggerly",
        "given": "Keith"
      },
      {
        "family": "Irizarry",
        "given": "Rafael A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2010,
          9,
          14
        ]
      ]
    },
    "container-title": "Nature Reviews Genetics",
    "DOI": "10.1038/nrg2825",
    "volume": "11",
    "issue": "10",
    "page": "733-739",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Tackling the widespread and critical impact of batch effects in high-throughput data",
    "URL": "https://doi.org/cfr324",
    "PMCID": "PMC3880143",
    "PMID": "20838408",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1038/nrg2825"
  },
  {
    "type": "article-journal",
    "id": "lwg6sPLT",
    "author": [
      {
        "family": "Mardt",
        "given": "Andreas"
      },
      {
        "family": "Pasquali",
        "given": "Luca"
      },
      {
        "family": "Wu",
        "given": "Hao"
      },
      {
        "family": "Noé",
        "given": "Frank"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          1,
          2
        ]
      ]
    },
    "container-title": "Nature Communications",
    "DOI": "10.1038/s41467-017-02388-1",
    "volume": "9",
    "issue": "1",
    "page": "5",
    "publisher": "Springer Science and Business Media LLC",
    "title": "VAMPnets for deep learning of molecular kinetics",
    "URL": "https://doi.org/gcvf62",
    "PMCID": "PMC5750224",
    "PMID": "29295994",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s41467-017-02388-1"
  },
  {
    "type": "article-journal",
    "id": "WGfstNkj",
    "author": [
      {
        "family": "Nielsen",
        "given": "Alec A. K."
      },
      {
        "family": "Voigt",
        "given": "Christopher A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          8,
          7
        ]
      ]
    },
    "container-title": "Nature Communications",
    "DOI": "10.1038/s41467-018-05378-z",
    "volume": "9",
    "issue": "1",
    "page": "3135",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Deep learning to predict the lab-of-origin of engineered DNA",
    "URL": "https://doi.org/gd27sw",
    "PMCID": "PMC6081423",
    "PMID": "30087331",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s41467-018-05378-z"
  },
  {
    "type": "article-journal",
    "id": "GdO9NZJH",
    "issued": {
      "date-parts": [
        [
          2018,
          10,
          10
        ]
      ]
    },
    "container-title": "Nature Biomedical Engineering",
    "DOI": "10.1038/s41551-018-0315-x",
    "volume": "2",
    "issue": "10",
    "page": "709-710",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Towards trustable machine learning",
    "URL": "https://doi.org/gfw9cn",
    "PMID": "31015650",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s41551-018-0315-x"
  },
  {
    "type": "article-journal",
    "id": "VpgPDZxv",
    "author": [
      {
        "family": "Byrd",
        "given": "James Brian"
      },
      {
        "family": "Greene",
        "given": "Anna C."
      },
      {
        "family": "Prasad",
        "given": "Deepashree Venkatesh"
      },
      {
        "family": "Jiang",
        "given": "Xiaoqian"
      },
      {
        "family": "Greene",
        "given": "Casey S."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          7,
          21
        ]
      ]
    },
    "container-title": "Nature Reviews Genetics",
    "DOI": "10.1038/s41576-020-0257-5",
    "volume": "21",
    "issue": "10",
    "page": "615-629",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Responsible, practical genomic data sharing that accelerates research",
    "URL": "https://doi.org/gg7c57",
    "PMID": "32694666",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s41576-020-0257-5"
  },
  {
    "type": "article-journal",
    "id": "NAEDY6H8",
    "author": [
      {
        "family": "Lillicrap",
        "given": "Timothy P."
      },
      {
        "family": "Santoro",
        "given": "Adam"
      },
      {
        "family": "Marris",
        "given": "Luke"
      },
      {
        "family": "Akerman",
        "given": "Colin J."
      },
      {
        "family": "Hinton",
        "given": "Geoffrey"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          4,
          17
        ]
      ]
    },
    "container-title": "Nature Reviews Neuroscience",
    "DOI": "10.1038/s41583-020-0277-3",
    "volume": "21",
    "issue": "6",
    "page": "335-346",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Backpropagation and the brain",
    "URL": "https://doi.org/ggsc7t",
    "PMID": "32303713",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s41583-020-0277-3"
  },
  {
    "type": "article-journal",
    "id": "8vmpDcPH",
    "author": [
      {
        "family": "Gurovich",
        "given": "Yaron"
      },
      {
        "family": "Hanani",
        "given": "Yair"
      },
      {
        "family": "Bar",
        "given": "Omri"
      },
      {
        "family": "Nadav",
        "given": "Guy"
      },
      {
        "family": "Fleischer",
        "given": "Nicole"
      },
      {
        "family": "Gelbman",
        "given": "Dekel"
      },
      {
        "family": "Basel-Salmon",
        "given": "Lina"
      },
      {
        "family": "Krawitz",
        "given": "Peter M."
      },
      {
        "family": "Kamphausen",
        "given": "Susanne B."
      },
      {
        "family": "Zenker",
        "given": "Martin"
      },
      {
        "family": "Bird",
        "given": "Lynne M."
      },
      {
        "family": "Gripp",
        "given": "Karen W."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019,
          1,
          7
        ]
      ]
    },
    "container-title": "Nature Medicine",
    "DOI": "10.1038/s41591-018-0279-0",
    "volume": "25",
    "issue": "1",
    "page": "60-64",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Identifying facial phenotypes of genetic disorders using deep learning",
    "URL": "https://doi.org/czdm",
    "PMID": "30617323",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s41591-018-0279-0"
  },
  {
    "type": "article-journal",
    "id": "WygOgk00",
    "author": [
      {
        "family": "Zhou",
        "given": "Zhenpeng"
      },
      {
        "family": "Kearnes",
        "given": "Steven"
      },
      {
        "family": "Li",
        "given": "Li"
      },
      {
        "family": "Zare",
        "given": "Richard N."
      },
      {
        "family": "Riley",
        "given": "Patrick"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019,
          7,
          24
        ]
      ]
    },
    "container-title": "Scientific Reports",
    "DOI": "10.1038/s41598-019-47148-x",
    "volume": "9",
    "issue": "1",
    "page": "10752",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Optimization of Molecules via Deep Reinforcement Learning",
    "URL": "https://doi.org/ggfqc8",
    "PMCID": "PMC6656766",
    "PMID": "31341196",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s41598-019-47148-x"
  },
  {
    "type": "article-journal",
    "id": "1DssZebFm",
    "author": [
      {
        "family": "Rajkomar",
        "given": "Alvin"
      },
      {
        "family": "Oren",
        "given": "Eyal"
      },
      {
        "family": "Chen",
        "given": "Kai"
      },
      {
        "family": "Dai",
        "given": "Andrew M."
      },
      {
        "family": "Hajaj",
        "given": "Nissan"
      },
      {
        "family": "Hardt",
        "given": "Michaela"
      },
      {
        "family": "Liu",
        "given": "Peter J."
      },
      {
        "family": "Liu",
        "given": "Xiaobing"
      },
      {
        "family": "Marcus",
        "given": "Jake"
      },
      {
        "family": "Sun",
        "given": "Mimi"
      },
      {
        "family": "Sundberg",
        "given": "Patrik"
      },
      {
        "family": "Yee",
        "given": "Hector"
      },
      {
        "family": "Zhang",
        "given": "Kun"
      },
      {
        "family": "Zhang",
        "given": "Yi"
      },
      {
        "family": "Flores",
        "given": "Gerardo"
      },
      {
        "family": "Duggan",
        "given": "Gavin E."
      },
      {
        "family": "Irvine",
        "given": "Jamie"
      },
      {
        "family": "Le",
        "given": "Quoc"
      },
      {
        "family": "Litsch",
        "given": "Kurt"
      },
      {
        "family": "Mossin",
        "given": "Alexander"
      },
      {
        "family": "Tansuwan",
        "given": "Justin"
      },
      {
        "family": "Wang",
        "given": "De"
      },
      {
        "family": "Wexler",
        "given": "James"
      },
      {
        "family": "Wilson",
        "given": "Jimbo"
      },
      {
        "family": "Ludwig",
        "given": "Dana"
      },
      {
        "family": "Volchenboum",
        "given": "Samuel L."
      },
      {
        "family": "Chou",
        "given": "Katherine"
      },
      {
        "family": "Pearson",
        "given": "Michael"
      },
      {
        "family": "Madabushi",
        "given": "Srinivasan"
      },
      {
        "family": "Shah",
        "given": "Nigam H."
      },
      {
        "family": "Butte",
        "given": "Atul J."
      },
      {
        "family": "Howell",
        "given": "Michael D."
      },
      {
        "family": "Cui",
        "given": "Claire"
      },
      {
        "family": "Corrado",
        "given": "Greg S."
      },
      {
        "family": "Dean",
        "given": "Jeffrey"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          5,
          8
        ]
      ]
    },
    "container-title": "npj Digital Medicine",
    "DOI": "10.1038/s41746-018-0029-1",
    "volume": "1",
    "issue": "1",
    "page": "18",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Scalable and accurate deep learning with electronic health records",
    "URL": "https://doi.org/gdqcc8",
    "PMCID": "PMC6550175",
    "PMID": "31304302",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s41746-018-0029-1"
  },
  {
    "type": "article-journal",
    "id": "lvjgHDOe",
    "author": [
      {
        "family": "Chen",
        "given": "David"
      },
      {
        "family": "Liu",
        "given": "Sijia"
      },
      {
        "family": "Kingsbury",
        "given": "Paul"
      },
      {
        "family": "Sohn",
        "given": "Sunghwan"
      },
      {
        "family": "Storlie",
        "given": "Curtis B."
      },
      {
        "family": "Habermann",
        "given": "Elizabeth B."
      },
      {
        "family": "Naessens",
        "given": "James M."
      },
      {
        "family": "Larson",
        "given": "David W."
      },
      {
        "family": "Liu",
        "given": "Hongfang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019,
          5,
          30
        ]
      ]
    },
    "container-title": "npj Digital Medicine",
    "DOI": "10.1038/s41746-019-0122-0",
    "volume": "2",
    "issue": "1",
    "page": "43",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Deep learning and alternative learning strategies for retrospective real-world clinical data",
    "URL": "https://doi.org/ghfwhh",
    "PMCID": "PMC6550223",
    "PMID": "31304389",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s41746-019-0122-0"
  },
  {
    "type": "article-journal",
    "id": "nqeUDzJ4",
    "author": [
      {
        "family": "Luo",
        "given": "Yunan"
      },
      {
        "family": "Peng",
        "given": "Jian"
      },
      {
        "family": "Ma",
        "given": "Jianzhu"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          8,
          12
        ]
      ]
    },
    "container-title": "Nature Machine Intelligence",
    "DOI": "10.1038/s42256-020-0218-x",
    "volume": "2",
    "issue": "8",
    "page": "426-427",
    "publisher": "Springer Science and Business Media LLC",
    "title": "When causal inference meets deep learning",
    "URL": "https://doi.org/ghfwxq",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1038/s42256-020-0218-x"
  },
  {
    "type": "article-journal",
    "id": "qCKLXDUQ",
    "author": [
      {
        "family": "Belkin",
        "given": "Mikhail"
      },
      {
        "family": "Hsu",
        "given": "Daniel"
      },
      {
        "family": "Ma",
        "given": "Siyuan"
      },
      {
        "family": "Mandal",
        "given": "Soumik"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019,
          8,
          6
        ]
      ]
    },
    "abstract": "Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.",
    "container-title": "Proceedings of the National Academy of Sciences",
    "DOI": "10.1073/pnas.1903070116",
    "volume": "116",
    "issue": "32",
    "page": "15849-15854",
    "publisher": "Proceedings of the National Academy of Sciences",
    "title": "Reconciling modern machine-learning practice and the classical bias–variance trade-off",
    "URL": "https://doi.org/gf5dmw",
    "PMCID": "PMC6689936",
    "PMID": "31341078",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1073/pnas.1903070116"
  },
  {
    "type": "article-journal",
    "id": "1AyQuG5x7",
    "author": [
      {
        "family": "Grapov",
        "given": "Dmitry"
      },
      {
        "family": "Fahrmann",
        "given": "Johannes"
      },
      {
        "family": "Wanichthanarak",
        "given": "Kwanjeera"
      },
      {
        "family": "Khoomrung",
        "given": "Sakda"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          10
        ]
      ]
    },
    "container-title": "OMICS: A Journal of Integrative Biology",
    "DOI": "10.1089/omi.2018.0097",
    "volume": "22",
    "issue": "10",
    "page": "630-636",
    "publisher": "Mary Ann Liebert Inc",
    "title": "Rise of Deep Learning for Genomic, Proteomic, and Metabolomic Data Integration in Precision Medicine",
    "URL": "https://doi.org/gfjjgn",
    "PMCID": "PMC6207407",
    "PMID": "30124358",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1089/omi.2018.0097"
  },
  {
    "type": "article-journal",
    "id": "QobI7Hyv",
    "author": [
      {
        "family": "Walsh",
        "given": "Ian"
      },
      {
        "family": "Pollastri",
        "given": "Gianluca"
      },
      {
        "family": "Tosatto",
        "given": "Silvio C. E."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016,
          9
        ]
      ]
    },
    "container-title": "Briefings in Bioinformatics",
    "DOI": "10.1093/bib/bbv082",
    "volume": "17",
    "issue": "5",
    "page": "831-840",
    "publisher": "Oxford University Press (OUP)",
    "title": "Correct machine learning on protein sequences: a peer-reviewing perspective",
    "URL": "https://doi.org/f89ms7",
    "PMID": "26411473",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1093/bib/bbv082"
  },
  {
    "type": "article-journal",
    "id": "1GGrbeMvT",
    "author": [
      {
        "family": "Lee",
        "given": "Alexandra J"
      },
      {
        "family": "Park",
        "given": "YoSon"
      },
      {
        "family": "Doing",
        "given": "Georgia"
      },
      {
        "family": "Hogan",
        "given": "Deborah A"
      },
      {
        "family": "Greene",
        "given": "Casey S"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          11,
          3
        ]
      ]
    },
    "container-title": "GigaScience",
    "DOI": "10.1093/gigascience/giaa117",
    "volume": "9",
    "issue": "11",
    "page": "giaa117",
    "publisher": "Oxford University Press (OUP)",
    "title": "Correcting for experiment-specific variability in expression compendia can remove underlying signals",
    "URL": "https://doi.org/ghhtpf",
    "PMCID": "PMC7607552",
    "PMID": "33140829",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1093/gigascience/giaa117"
  },
  {
    "type": "article-journal",
    "id": "kCSge2o8",
    "author": [
      {
        "family": "Cocos",
        "given": "Anne"
      },
      {
        "family": "Fiks",
        "given": "Alexander G"
      },
      {
        "family": "Masino",
        "given": "Aaron J"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          7
        ]
      ]
    },
    "container-title": "Journal of the American Medical Informatics Association",
    "DOI": "10.1093/jamia/ocw180",
    "volume": "24",
    "issue": "4",
    "page": "813-821",
    "publisher": "Oxford University Press (OUP)",
    "title": "Deep learning for pharmacovigilance: recurrent neural network architectures for labeling adverse drug reactions in Twitter posts",
    "URL": "https://doi.org/gbp9nj",
    "PMCID": "PMC7651964",
    "PMID": "28339747",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1093/jamia/ocw180"
  },
  {
    "type": "article-journal",
    "id": "PZMP42Ak",
    "author": [
      {
        "family": "Ching",
        "given": "Travers"
      },
      {
        "family": "Himmelstein",
        "given": "Daniel S."
      },
      {
        "family": "Beaulieu-Jones",
        "given": "Brett K."
      },
      {
        "family": "Kalinin",
        "given": "Alexandr A."
      },
      {
        "family": "Do",
        "given": "Brian T."
      },
      {
        "family": "Way",
        "given": "Gregory P."
      },
      {
        "family": "Ferrero",
        "given": "Enrico"
      },
      {
        "family": "Agapow",
        "given": "Paul-Michael"
      },
      {
        "family": "Zietz",
        "given": "Michael"
      },
      {
        "family": "Hoffman",
        "given": "Michael M."
      },
      {
        "family": "Xie",
        "given": "Wei"
      },
      {
        "family": "Rosen",
        "given": "Gail L."
      },
      {
        "family": "Lengerich",
        "given": "Benjamin J."
      },
      {
        "family": "Israeli",
        "given": "Johnny"
      },
      {
        "family": "Lanchantin",
        "given": "Jack"
      },
      {
        "family": "Woloszynek",
        "given": "Stephen"
      },
      {
        "family": "Carpenter",
        "given": "Anne E."
      },
      {
        "family": "Shrikumar",
        "given": "Avanti"
      },
      {
        "family": "Xu",
        "given": "Jinbo"
      },
      {
        "family": "Cofer",
        "given": "Evan M."
      },
      {
        "family": "Lavender",
        "given": "Christopher A."
      },
      {
        "family": "Turaga",
        "given": "Srinivas C."
      },
      {
        "family": "Alexandari",
        "given": "Amr M."
      },
      {
        "family": "Lu",
        "given": "Zhiyong"
      },
      {
        "family": "Harris",
        "given": "David J."
      },
      {
        "family": "DeCaprio",
        "given": "Dave"
      },
      {
        "family": "Qi",
        "given": "Yanjun"
      },
      {
        "family": "Kundaje",
        "given": "Anshul"
      },
      {
        "family": "Peng",
        "given": "Yifan"
      },
      {
        "family": "Wiley",
        "given": "Laura K."
      },
      {
        "family": "Segler",
        "given": "Marwin H. S."
      },
      {
        "family": "Boca",
        "given": "Simina M."
      },
      {
        "family": "Swamidass",
        "given": "S. Joshua"
      },
      {
        "family": "Huang",
        "given": "Austin"
      },
      {
        "family": "Gitter",
        "given": "Anthony"
      },
      {
        "family": "Greene",
        "given": "Casey S."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          4,
          4
        ]
      ]
    },
    "abstract": "Deep learning describes a class of machine learning algorithms that are capable of combining raw inputs into layers of intermediate features. These algorithms have recently shown impressive results across a variety of domains. Biology and medicine are data-rich disciplines, but the data are complex and often ill-understood. Hence, deep learning techniques may be particularly well suited to solve problems of these fields. We examine applications of deep learning to a variety of biomedical problems—patient classification, fundamental biological processes and treatment of patients—and discuss whether deep learning will be able to transform these tasks or if the biomedical sphere poses unique challenges. Following from an extensive literature review, we find that deep learning has yet to revolutionize biomedicine or definitively resolve any of the most pressing challenges in the field, but promising advances have been made on the prior state of the art. Even though improvements over previous baselines have been modest in general, the recent progress indicates that deep learning methods will provide valuable means for speeding up or aiding human investigation. Though progress has been made linking a specific neural network's prediction to input features, understanding how users should interpret these models to make testable hypotheses about the system under study remains an open challenge. Furthermore, the limited amount of labelled data for training presents problems in some domains, as do legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning enabling changes at both bench and bedside with the potential to transform several areas of biology and medicine.",
    "container-title": "Journal of The Royal Society Interface",
    "DOI": "10.1098/rsif.2017.0387",
    "volume": "15",
    "issue": "141",
    "page": "20170387",
    "publisher": "The Royal Society",
    "title": "Opportunities and obstacles for deep learning in biology and medicine",
    "URL": "https://doi.org/gddkhn",
    "PMCID": "PMC5938574",
    "PMID": "29618526",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1098/rsif.2017.0387"
  },
  {
    "type": "article-journal",
    "id": "fbIH12yd",
    "author": [
      {
        "family": "Beaulieu-Jones",
        "given": "Brett K."
      },
      {
        "family": "Wu",
        "given": "Zhiwei Steven"
      },
      {
        "family": "Williams",
        "given": "Chris"
      },
      {
        "family": "Lee",
        "given": "Ran"
      },
      {
        "family": "Bhavnani",
        "given": "Sanjeev P."
      },
      {
        "family": "Byrd",
        "given": "James Brian"
      },
      {
        "family": "Greene",
        "given": "Casey S."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          12,
          20
        ]
      ]
    },
    "container-title": "Cold Spring Harbor Laboratory",
    "DOI": "10.1101/159756",
    "publisher": "Cold Spring Harbor Laboratory",
    "title": "Privacy-preserving generative deep neural networks support clinical data sharing",
    "URL": "https://doi.org/gcnzrn",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1101/159756"
  },
  {
    "type": "article-journal",
    "id": "14cVrrqP1",
    "author": [
      {
        "family": "Avsec",
        "given": "Žiga"
      },
      {
        "family": "Kreuzhuber",
        "given": "Roman"
      },
      {
        "family": "Israeli",
        "given": "Johnny"
      },
      {
        "family": "Xu",
        "given": "Nancy"
      },
      {
        "family": "Cheng",
        "given": "Jun"
      },
      {
        "family": "Shrikumar",
        "given": "Avanti"
      },
      {
        "family": "Banerjee",
        "given": "Abhimanyu"
      },
      {
        "family": "Kim",
        "given": "Daniel S."
      },
      {
        "family": "Urban",
        "given": "Lara"
      },
      {
        "family": "Kundaje",
        "given": "Anshul"
      },
      {
        "family": "Stegle",
        "given": "Oliver"
      },
      {
        "family": "Gagneur",
        "given": "Julien"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          7,
          24
        ]
      ]
    },
    "abstract": "Advanced machine learning models applied to large-scale genomics datasets hold the promise to be major drivers for genome science. Once trained, such models can serve as a tool to probe the relationships between data modalities, including the effect of genetic variants on phenotype. However, lack of standardization and limited accessibility of trained models have hampered their impact in practice. To address this, we present Kipoi, a collaborative initiative to define standards and to foster reuse of trained models in genomics. Already, the Kipoi repository contains over 2,000 trained models that cover canonical prediction tasks in transcriptional and post-transcriptional gene regulation. The Kipoi model standard grants automated software installation and provides unified interfaces to apply and interpret models. We illustrate Kipoi through canonical use cases, including model benchmarking, transfer learning, variant effect prediction, and building new models from existing ones. By providing a unified framework to archive, share, access, use, and build on models developed by the community, Kipoi will foster the dissemination and use of machine learning models in genomics.",
    "container-title": "Cold Spring Harbor Laboratory",
    "DOI": "10.1101/375345",
    "publisher": "Cold Spring Harbor Laboratory",
    "title": "Kipoi: accelerating the community exchange and reuse of predictive models for genomics",
    "URL": "https://doi.org/gd24sx",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1101/375345"
  },
  {
    "type": "article-journal",
    "id": "3MV4PRAe",
    "author": [
      {
        "family": "Rives",
        "given": "Alexander"
      },
      {
        "family": "Meier",
        "given": "Joshua"
      },
      {
        "family": "Sercu",
        "given": "Tom"
      },
      {
        "family": "Goyal",
        "given": "Siddharth"
      },
      {
        "family": "Lin",
        "given": "Zeming"
      },
      {
        "family": "Liu",
        "given": "Jason"
      },
      {
        "family": "Guo",
        "given": "Demi"
      },
      {
        "family": "Ott",
        "given": "Myle"
      },
      {
        "family": "Zitnick",
        "given": "C. Lawrence"
      },
      {
        "family": "Ma",
        "given": "Jerry"
      },
      {
        "family": "Fergus",
        "given": "Rob"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          12,
          15
        ]
      ]
    },
    "abstract": "In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multi-scale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure, and improving state-of-the-art features for long-range contact prediction.",
    "container-title": "Cold Spring Harbor Laboratory",
    "DOI": "10.1101/622803",
    "publisher": "Cold Spring Harbor Laboratory",
    "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
    "URL": "https://doi.org/gf2x4p",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1101/622803"
  },
  {
    "type": "article-journal",
    "id": "x7a5SM90",
    "author": [
      {
        "family": "Razavian",
        "given": "Ali Sharif"
      },
      {
        "family": "Azizpour",
        "given": "Hossein"
      },
      {
        "family": "Sullivan",
        "given": "Josephine"
      },
      {
        "family": "Carlsson",
        "given": "Stefan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2014,
          6
        ]
      ]
    },
    "container-title": "Institute of Electrical and Electronics Engineers (IEEE)",
    "DOI": "10.1109/cvprw.2014.131",
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "title": "CNN Features Off-the-Shelf: An Astounding Baseline for Recognition",
    "URL": "https://doi.org/f3np4s",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1109/cvprw.2014.131"
  },
  {
    "type": "article-journal",
    "id": "v9nPZ4kH",
    "author": [
      {
        "family": "Oussidi",
        "given": "Achraf"
      },
      {
        "family": "Elhassouny",
        "given": "Azeddine"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          4
        ]
      ]
    },
    "container-title": "Institute of Electrical and Electronics Engineers (IEEE)",
    "DOI": "10.1109/isacv.2018.8354080",
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "title": "Deep generative models: Survey",
    "URL": "https://doi.org/ghjtg7",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1109/isacv.2018.8354080"
  },
  {
    "type": "article-journal",
    "id": "8seWxxzY",
    "author": [
      {
        "family": "Ravi",
        "given": "Daniele"
      },
      {
        "family": "Wong",
        "given": "Charence"
      },
      {
        "family": "Deligianni",
        "given": "Fani"
      },
      {
        "family": "Berthelot",
        "given": "Melissa"
      },
      {
        "family": "Andreu-Perez",
        "given": "Javier"
      },
      {
        "family": "Lo",
        "given": "Benny"
      },
      {
        "family": "Yang",
        "given": "Guang-Zhong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          1
        ]
      ]
    },
    "container-title": "IEEE Journal of Biomedical and Health Informatics",
    "DOI": "10.1109/jbhi.2016.2636665",
    "volume": "21",
    "issue": "1",
    "page": "4-21",
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "title": "Deep Learning for Health Informatics",
    "URL": "https://doi.org/gfgtzx",
    "PMID": "28055930",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1109/jbhi.2016.2636665"
  },
  {
    "type": "article-journal",
    "id": "L7EocHX2",
    "author": [
      {
        "family": "Sze",
        "given": "Vivienne"
      },
      {
        "family": "Chen",
        "given": "Yu-Hsin"
      },
      {
        "family": "Yang",
        "given": "Tien-Ju"
      },
      {
        "family": "Emer",
        "given": "Joel S."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          12
        ]
      ]
    },
    "container-title": "Proceedings of the IEEE",
    "DOI": "10.1109/jproc.2017.2761740",
    "volume": "105",
    "issue": "12",
    "page": "2295-2329",
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "title": "Efficient Processing of Deep Neural Networks: A Tutorial and Survey",
    "URL": "https://doi.org/gcnp38",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1109/jproc.2017.2761740"
  },
  {
    "type": "article-journal",
    "id": "ZwUaSNWa",
    "author": [
      {
        "family": "Zhang",
        "given": "Wenlu"
      },
      {
        "family": "Li",
        "given": "Rongjian"
      },
      {
        "family": "Zeng",
        "given": "Tao"
      },
      {
        "family": "Sun",
        "given": "Qian"
      },
      {
        "family": "Kumar",
        "given": "Sudhir"
      },
      {
        "family": "Ye",
        "given": "Jieping"
      },
      {
        "family": "Ji",
        "given": "Shuiwang"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          6,
          1
        ]
      ]
    },
    "container-title": "IEEE Transactions on Big Data",
    "DOI": "10.1109/tbdata.2016.2573280",
    "volume": "6",
    "issue": "2",
    "page": "322-333",
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "title": "Deep Model Based Transfer and Multi-Task Learning for Biological Image Analysis",
    "URL": "https://doi.org/gfvs28",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1109/tbdata.2016.2573280"
  },
  {
    "type": "article-journal",
    "id": "lBFmt4aO",
    "author": [
      {
        "family": "Ferreira",
        "given": "André C."
      },
      {
        "family": "Silva",
        "given": "Liliana R."
      },
      {
        "family": "Renna",
        "given": "Francesco"
      },
      {
        "family": "Brandl",
        "given": "Hanja B."
      },
      {
        "family": "Renoult",
        "given": "Julien P."
      },
      {
        "family": "Farine",
        "given": "Damien R."
      },
      {
        "family": "Covas",
        "given": "Rita"
      },
      {
        "family": "Doutrelant",
        "given": "Claire"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          7,
          26
        ]
      ]
    },
    "container-title": "Methods in Ecology and Evolution",
    "DOI": "10.1111/2041-210x.13436",
    "volume": "11",
    "issue": "9",
    "page": "1072-1085",
    "publisher": "Wiley",
    "title": "Deep learning‐based methods for individual recognition in small birds",
    "URL": "https://doi.org/d438",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1111/2041-210x.13436"
  },
  {
    "type": "article-journal",
    "id": "gTcMnARc",
    "author": [
      {
        "family": "Hu",
        "given": "Qiwen"
      },
      {
        "family": "Greene",
        "given": "Casey S."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          11
        ]
      ]
    },
    "container-title": "World Scientific Pub Co Pte Lt",
    "DOI": "10.1142/9789813279827_0033",
    "publisher": "World Scientific Pub Co Pte Lt",
    "title": "Parameter tuning is a key part of dimensionality reduction via deep variational autoencoders for single cell RNA transcriptomics",
    "URL": "https://doi.org/gf5pc7",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1142/9789813279827_0033"
  },
  {
    "type": "article-journal",
    "id": "gSmt16Rh",
    "author": [
      {
        "family": "Caruana",
        "given": "Rich"
      },
      {
        "family": "Lou",
        "given": "Yin"
      },
      {
        "family": "Gehrke",
        "given": "Johannes"
      },
      {
        "family": "Koch",
        "given": "Paul"
      },
      {
        "family": "Sturm",
        "given": "Marc"
      },
      {
        "family": "Elhadad",
        "given": "Noemie"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2015,
          8,
          10
        ]
      ]
    },
    "container-title": "Association for Computing Machinery (ACM)",
    "DOI": "10.1145/2783258.2788613",
    "publisher": "Association for Computing Machinery (ACM)",
    "title": "Intelligible Models for HealthCare",
    "URL": "https://doi.org/gftgxk",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1145/2783258.2788613"
  },
  {
    "type": "article-journal",
    "id": "zCqhgXvY",
    "author": [
      {
        "family": "Fredrikson",
        "given": "Matt"
      },
      {
        "family": "Jha",
        "given": "Somesh"
      },
      {
        "family": "Ristenpart",
        "given": "Thomas"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2015,
          10,
          12
        ]
      ]
    },
    "container-title": "Association for Computing Machinery (ACM)",
    "DOI": "10.1145/2810103.2813677",
    "publisher": "Association for Computing Machinery (ACM)",
    "title": "Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures",
    "URL": "https://doi.org/cwdm",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1145/2810103.2813677"
  },
  {
    "type": "article-journal",
    "id": "LiCxcgZp",
    "author": [
      {
        "family": "Abadi",
        "given": "Martin"
      },
      {
        "family": "Chu",
        "given": "Andy"
      },
      {
        "family": "Goodfellow",
        "given": "Ian"
      },
      {
        "family": "McMahan",
        "given": "H. Brendan"
      },
      {
        "family": "Mironov",
        "given": "Ilya"
      },
      {
        "family": "Talwar",
        "given": "Kunal"
      },
      {
        "family": "Zhang",
        "given": "Li"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016,
          10,
          24
        ]
      ]
    },
    "container-title": "Association for Computing Machinery (ACM)",
    "DOI": "10.1145/2976749.2978318",
    "publisher": "Association for Computing Machinery (ACM)",
    "title": "Deep Learning with Differential Privacy",
    "URL": "https://doi.org/gcrnp3",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1145/2976749.2978318"
  },
  {
    "type": "article-journal",
    "id": "fVSo2gZU",
    "author": [
      {
        "family": "Krizhevsky",
        "given": "Alex"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      },
      {
        "family": "Hinton",
        "given": "Geoffrey E."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          5,
          24
        ]
      ]
    },
    "container-title": "Communications of the ACM",
    "DOI": "10.1145/3065386",
    "volume": "60",
    "issue": "6",
    "page": "84-90",
    "publisher": "Association for Computing Machinery (ACM)",
    "title": "ImageNet classification with deep convolutional neural networks",
    "URL": "https://doi.org/gbhhxs",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1145/3065386"
  },
  {
    "type": "article-journal",
    "id": "TqPn1DCX",
    "author": [
      {
        "family": "Mitchell",
        "given": "Margaret"
      },
      {
        "family": "Wu",
        "given": "Simone"
      },
      {
        "family": "Zaldivar",
        "given": "Andrew"
      },
      {
        "family": "Barnes",
        "given": "Parker"
      },
      {
        "family": "Vasserman",
        "given": "Lucy"
      },
      {
        "family": "Hutchinson",
        "given": "Ben"
      },
      {
        "family": "Spitzer",
        "given": "Elena"
      },
      {
        "family": "Raji",
        "given": "Inioluwa Deborah"
      },
      {
        "family": "Gebru",
        "given": "Timnit"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019,
          1,
          29
        ]
      ]
    },
    "container-title": "Association for Computing Machinery (ACM)",
    "DOI": "10.1145/3287560.3287596",
    "publisher": "Association for Computing Machinery (ACM)",
    "title": "Model Cards for Model Reporting",
    "URL": "https://doi.org/gftgjg",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1145/3287560.3287596"
  },
  {
    "type": "article-journal",
    "id": "hJQdIoO3",
    "author": [
      {
        "family": "Dietterich",
        "given": "Thomas G."
      }
    ],
    "issued": {
      "date-parts": [
        [
          1998,
          10,
          1
        ]
      ]
    },
    "abstract": "This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These test sare compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type I error). Two widely used statistical tests are shown to have high probability of type I error in certain situations and should never be used: a test for the difference of two proportions and a paired-differences t test based on taking several random train-test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of type I error. A fourth test, McNemar's test, is shown to have low type I error. The fifth test is a new test, 5 × 2 cv, based on five iterations of twofold cross-validation. Experiments show that this test also has acceptable type I error. The article also measures the power (ability to detect algorithm differences when they do exist) of these tests. The cross-validated t test is the most powerful. The 5×2 cv test is shown to be slightly more powerful than McNemar's test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only once, Mc-Nemar's test is the only test with acceptable type I error. For algorithms that can be executed 10 times, the 5 × 2 cv test is recommended, because it is slightly more powerful and because it directly measures variation due to the choice of training set.",
    "container-title": "Neural Computation",
    "DOI": "10.1162/089976698300017197",
    "volume": "10",
    "issue": "7",
    "page": "1895-1923",
    "publisher": "MIT Press - Journals",
    "title": "Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms",
    "URL": "https://doi.org/fqc9w5",
    "PMID": "9744903",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1162/089976698300017197"
  },
  {
    "type": "article-journal",
    "id": "p4Nl5If0",
    "author": [
      {
        "family": "Chicco",
        "given": "Davide"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          12,
          8
        ]
      ]
    },
    "container-title": "BioData Mining",
    "DOI": "10.1186/s13040-017-0155-3",
    "volume": "10",
    "issue": "1",
    "page": "35",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Ten quick tips for machine learning in computational biology",
    "URL": "https://doi.org/gdb9wr",
    "PMCID": "PMC5721660",
    "PMID": "29234465",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1186/s13040-017-0155-3"
  },
  {
    "type": "article-journal",
    "id": "19zfIm033",
    "author": [
      {
        "family": "Koutsoukas",
        "given": "Alexios"
      },
      {
        "family": "Monaghan",
        "given": "Keith J."
      },
      {
        "family": "Li",
        "given": "Xiaoli"
      },
      {
        "family": "Huan",
        "given": "Jun"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          6,
          28
        ]
      ]
    },
    "container-title": "Journal of Cheminformatics",
    "DOI": "10.1186/s13321-017-0226-y",
    "volume": "9",
    "issue": "1",
    "page": "42",
    "publisher": "Springer Science and Business Media LLC",
    "title": "Deep-learning: investigating deep neural networks hyper-parameters and comparison of performance to shallow methods for modeling bioactivity data",
    "URL": "https://doi.org/gfwv4d",
    "PMCID": "PMC5489441",
    "PMID": "29086090",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1186/s13321-017-0226-y"
  },
  {
    "type": "article-journal",
    "id": "SI1X6npH",
    "author": [
      {
        "family": "Rivenson",
        "given": "Yair"
      },
      {
        "family": "Göröcs",
        "given": "Zoltán"
      },
      {
        "family": "Günaydin",
        "given": "Harun"
      },
      {
        "family": "Zhang",
        "given": "Yibo"
      },
      {
        "family": "Wang",
        "given": "Hongda"
      },
      {
        "family": "Ozcan",
        "given": "Aydogan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          11,
          20
        ]
      ]
    },
    "container-title": "Optica",
    "DOI": "10.1364/optica.4.001437",
    "volume": "4",
    "issue": "11",
    "page": "1437",
    "publisher": "The Optical Society",
    "title": "Deep learning microscopy",
    "URL": "https://doi.org/gf8dhj",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1364/optica.4.001437"
  },
  {
    "type": "article-journal",
    "id": "Pf3steOn",
    "author": [
      {
        "family": "Sandve",
        "given": "Geir Kjetil"
      },
      {
        "family": "Nekrutenko",
        "given": "Anton"
      },
      {
        "family": "Taylor",
        "given": "James"
      },
      {
        "family": "Hovig",
        "given": "Eivind"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2013,
          10,
          24
        ]
      ]
    },
    "container-title": "PLoS Computational Biology",
    "DOI": "10.1371/journal.pcbi.1003285",
    "volume": "9",
    "issue": "10",
    "page": "e1003285",
    "publisher": "Public Library of Science (PLoS)",
    "title": "Ten Simple Rules for Reproducible Computational Research",
    "URL": "https://doi.org/pjb",
    "PMCID": "PMC3812051",
    "PMID": "24204232",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1371/journal.pcbi.1003285"
  },
  {
    "type": "article-journal",
    "id": "kEX5dgzK",
    "author": [
      {
        "family": "Perez-Riverol",
        "given": "Yasset"
      },
      {
        "family": "Gatto",
        "given": "Laurent"
      },
      {
        "family": "Wang",
        "given": "Rui"
      },
      {
        "family": "Sachsenberg",
        "given": "Timo"
      },
      {
        "family": "Uszkoreit",
        "given": "Julian"
      },
      {
        "family": "Leprevost",
        "given": "Felipe da Veiga"
      },
      {
        "family": "Fufezan",
        "given": "Christian"
      },
      {
        "family": "Ternent",
        "given": "Tobias"
      },
      {
        "family": "Eglen",
        "given": "Stephen J."
      },
      {
        "family": "Katz",
        "given": "Daniel S."
      },
      {
        "family": "Pollard",
        "given": "Tom J."
      },
      {
        "family": "Konovalov",
        "given": "Alexander"
      },
      {
        "family": "Flight",
        "given": "Robert M."
      },
      {
        "family": "Blin",
        "given": "Kai"
      },
      {
        "family": "Vizcaíno",
        "given": "Juan Antonio"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2016,
          7,
          14
        ]
      ]
    },
    "container-title": "PLOS Computational Biology",
    "DOI": "10.1371/journal.pcbi.1004947",
    "volume": "12",
    "issue": "7",
    "page": "e1004947",
    "publisher": "Public Library of Science (PLoS)",
    "title": "Ten Simple Rules for Taking Advantage of Git and GitHub",
    "URL": "https://doi.org/gbrb39",
    "PMCID": "PMC4945047",
    "PMID": "27415786",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1371/journal.pcbi.1004947"
  },
  {
    "type": "article-journal",
    "id": "uXPlMpfq",
    "author": [
      {
        "family": "Zook",
        "given": "Matthew"
      },
      {
        "family": "Barocas",
        "given": "Solon"
      },
      {
        "family": "boyd",
        "given": "danah"
      },
      {
        "family": "Crawford",
        "given": "Kate"
      },
      {
        "family": "Keller",
        "given": "Emily"
      },
      {
        "family": "Gangadharan",
        "given": "Seeta Peña"
      },
      {
        "family": "Goodman",
        "given": "Alyssa"
      },
      {
        "family": "Hollander",
        "given": "Rachelle"
      },
      {
        "family": "Koenig",
        "given": "Barbara A."
      },
      {
        "family": "Metcalf",
        "given": "Jacob"
      },
      {
        "family": "Narayanan",
        "given": "Arvind"
      },
      {
        "family": "Nelson",
        "given": "Alondra"
      },
      {
        "family": "Pasquale",
        "given": "Frank"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          3,
          30
        ]
      ]
    },
    "container-title": "PLOS Computational Biology",
    "DOI": "10.1371/journal.pcbi.1005399",
    "volume": "13",
    "issue": "3",
    "page": "e1005399",
    "publisher": "Public Library of Science (PLoS)",
    "title": "Ten simple rules for responsible big data research",
    "URL": "https://doi.org/gdqfcn",
    "PMCID": "PMC5373508",
    "PMID": "28358831",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1371/journal.pcbi.1005399"
  },
  {
    "type": "article-journal",
    "id": "me326jb9",
    "author": [
      {
        "family": "Titus",
        "given": "Alexander J."
      },
      {
        "family": "Flower",
        "given": "Audrey"
      },
      {
        "family": "Hagerty",
        "given": "Patrick"
      },
      {
        "family": "Gamble",
        "given": "Paul"
      },
      {
        "family": "Lewis",
        "given": "Charlie"
      },
      {
        "family": "Stavish",
        "given": "Todd"
      },
      {
        "family": "O’Connell",
        "given": "Kevin P."
      },
      {
        "family": "Shipley",
        "given": "Greg"
      },
      {
        "family": "Rogers",
        "given": "Stephanie M."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          9,
          4
        ]
      ]
    },
    "container-title": "PLOS Computational Biology",
    "DOI": "10.1371/journal.pcbi.1006454",
    "volume": "14",
    "issue": "9",
    "page": "e1006454",
    "publisher": "Public Library of Science (PLoS)",
    "title": "SIG-DB: Leveraging homomorphic encryption to securely interrogate privately held genomic databases",
    "URL": "https://doi.org/gd6xd5",
    "PMCID": "PMC6138421",
    "PMID": "30180163",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1371/journal.pcbi.1006454"
  },
  {
    "type": "article-journal",
    "id": "YuJbg3zO",
    "author": [
      {
        "family": "Himmelstein",
        "given": "Daniel S."
      },
      {
        "family": "Rubinetti",
        "given": "Vincent"
      },
      {
        "family": "Slochower",
        "given": "David R."
      },
      {
        "family": "Hu",
        "given": "Dongbo"
      },
      {
        "family": "Malladi",
        "given": "Venkat S."
      },
      {
        "family": "Greene",
        "given": "Casey S."
      },
      {
        "family": "Gitter",
        "given": "Anthony"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019,
          6,
          24
        ]
      ]
    },
    "container-title": "PLOS Computational Biology",
    "DOI": "10.1371/journal.pcbi.1007128",
    "volume": "15",
    "issue": "6",
    "page": "e1007128",
    "publisher": "Public Library of Science (PLoS)",
    "title": "Open collaborative writing with Manubot",
    "URL": "https://doi.org/c7np",
    "PMCID": "PMC6611653",
    "PMID": "31233491",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1371/journal.pcbi.1007128"
  },
  {
    "type": "article-journal",
    "id": "NDyhvXoh",
    "author": [
      {
        "family": "Zech",
        "given": "John R."
      },
      {
        "family": "Badgeley",
        "given": "Marcus A."
      },
      {
        "family": "Liu",
        "given": "Manway"
      },
      {
        "family": "Costa",
        "given": "Anthony B."
      },
      {
        "family": "Titano",
        "given": "Joseph J."
      },
      {
        "family": "Oermann",
        "given": "Eric Karl"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          11,
          6
        ]
      ]
    },
    "container-title": "PLOS Medicine",
    "DOI": "10.1371/journal.pmed.1002683",
    "volume": "15",
    "issue": "11",
    "page": "e1002683",
    "publisher": "Public Library of Science (PLoS)",
    "title": "Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study",
    "URL": "https://doi.org/gfj53h",
    "PMCID": "PMC6219764",
    "PMID": "30399157",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1371/journal.pmed.1002683"
  },
  {
    "type": "article-journal",
    "id": "su90EPNJ",
    "author": [
      {
        "family": "Cohen",
        "given": "I. Glenn"
      },
      {
        "family": "Amarasingham",
        "given": "Ruben"
      },
      {
        "family": "Shah",
        "given": "Anand"
      },
      {
        "family": "Xie",
        "given": "Bin"
      },
      {
        "family": "Lo",
        "given": "Bernard"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2014,
          7
        ]
      ]
    },
    "container-title": "Health Affairs",
    "DOI": "10.1377/hlthaff.2014.0048",
    "volume": "33",
    "issue": "7",
    "page": "1139-1147",
    "publisher": "Health Affairs (Project Hope)",
    "title": "The Legal And Ethical Concerns That Arise From Using Complex Predictive Analytics In Health Care",
    "URL": "https://doi.org/f6dggf",
    "PMID": "25006139",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1377/hlthaff.2014.0048"
  },
  {
    "type": "article-journal",
    "id": "10UmE5yi5",
    "author": [
      {
        "family": "Gundersen",
        "given": "Odd Erik"
      },
      {
        "family": "Gil",
        "given": "Yolanda"
      },
      {
        "family": "Aha",
        "given": "David W."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          9,
          28
        ]
      ]
    },
    "abstract": "Background: Science is experiencing a reproducibility crisis. Artificial intelligence research is not an exception. Objective: To give practical and pragmatic recommendations for how to document AI research so that the results are reproducible. Method: Our analysis of the literature shows that AI publications fall short of providing enough documentation to facilitate reproducibility. Our suggested best practices are based on a framework for reproducibility and recommendations given for other disciplines. Results: We have made an author checklist based on our investigation and provided examples for how every item in the checklist can be documented. Conclusion: We encourage reviewers to use the suggested best practices and author checklist when reviewing submissions for AAAI publications and future AAAI conferences.",
    "container-title": "AI Magazine",
    "DOI": "10.1609/aimag.v39i3.2816",
    "volume": "39",
    "issue": "3",
    "page": "56-68",
    "publisher": "Association for the Advancement of Artificial Intelligence (AAAI)",
    "title": "On Reproducible AI: Towards Reproducible Research, Open Science, and Digital Scholarship in AI Publications",
    "URL": "https://doi.org/gfcqt6",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.1609/aimag.v39i3.2816"
  },
  {
    "type": "article-journal",
    "id": "hqd50JaU",
    "author": [
      {
        "family": "Wolf",
        "given": "Thomas"
      },
      {
        "family": "Debut",
        "given": "Lysandre"
      },
      {
        "family": "Sanh",
        "given": "Victor"
      },
      {
        "family": "Chaumond",
        "given": "Julien"
      },
      {
        "family": "Delangue",
        "given": "Clement"
      },
      {
        "family": "Moi",
        "given": "Anthony"
      },
      {
        "family": "Cistac",
        "given": "Pierric"
      },
      {
        "family": "Rault",
        "given": "Tim"
      },
      {
        "family": "Louf",
        "given": "Remi"
      },
      {
        "family": "Funtowicz",
        "given": "Morgan"
      },
      {
        "family": "Davison",
        "given": "Joe"
      },
      {
        "family": "Shleifer",
        "given": "Sam"
      },
      {
        "family": "von Platen",
        "given": "Patrick"
      },
      {
        "family": "Ma",
        "given": "Clara"
      },
      {
        "family": "Jernite",
        "given": "Yacine"
      },
      {
        "family": "Plu",
        "given": "Julien"
      },
      {
        "family": "Xu",
        "given": "Canwen"
      },
      {
        "family": "Le Scao",
        "given": "Teven"
      },
      {
        "family": "Gugger",
        "given": "Sylvain"
      },
      {
        "family": "Drame",
        "given": "Mariama"
      },
      {
        "family": "Lhoest",
        "given": "Quentin"
      },
      {
        "family": "Rush",
        "given": "Alexander"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020
        ]
      ]
    },
    "container-title": "Association for Computational Linguistics (ACL)",
    "DOI": "10.18653/v1/2020.emnlp-demos.6",
    "publisher": "Association for Computational Linguistics (ACL)",
    "title": "Transformers: State-of-the-Art Natural Language Processing",
    "URL": "https://doi.org/ghs3bd",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.18653/v1/2020.emnlp-demos.6"
  },
  {
    "type": "article-journal",
    "id": "S3wNg1If",
    "author": [
      {
        "family": "Lin",
        "given": "Eugene"
      },
      {
        "family": "Kuo",
        "given": "Po-Hsiu"
      },
      {
        "family": "Liu",
        "given": "Yu-Li"
      },
      {
        "family": "Yu",
        "given": "Younger W.-Y."
      },
      {
        "family": "Yang",
        "given": "Albert C."
      },
      {
        "family": "Tsai",
        "given": "Shih-Jen"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2018,
          7,
          6
        ]
      ]
    },
    "container-title": "Frontiers in Psychiatry",
    "DOI": "10.3389/fpsyt.2018.00290",
    "volume": "9",
    "page": "290",
    "publisher": "Frontiers Media SA",
    "title": "A Deep Learning Approach for Predicting Antidepressant Response in Major Depression Using Clinical and Genetic Biomarkers",
    "URL": "https://doi.org/gdv7r2",
    "PMCID": "PMC6043864",
    "PMID": "30034349",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.3389/fpsyt.2018.00290"
  },
  {
    "type": "article-journal",
    "id": "lyJaUNDq",
    "author": [
      {
        "family": "Bemister-Buffington",
        "given": "Joseph"
      },
      {
        "family": "Wolf",
        "given": "Alex J."
      },
      {
        "family": "Raschka",
        "given": "Sebastian"
      },
      {
        "family": "Kuhn",
        "given": "Leslie A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          3,
          14
        ]
      ]
    },
    "abstract": "We show that machine learning can pinpoint features distinguishing inactive from active states in proteins, in particular identifying key ligand binding site flexibility transitions in GPCRs that are triggered by biologically active ligands. Our analysis was performed on the helical segments and loops in 18 inactive and 9 active class A G protein-coupled receptors (GPCRs). These three-dimensional (3D) structures were determined in complex with ligands. However, considering the flexible versus rigid state identified by graph-theoretic ProFlex rigidity analysis for each helix and loop segment with the ligand removed, followed by feature selection and k-nearest neighbor classification, was sufficient to identify four segments surrounding the ligand binding site whose flexibility/rigidity accurately predicts whether a GPCR is in an active or inactive state. GPCRs bound to inhibitors were similar in their pattern of flexible versus rigid regions, whereas agonist-bound GPCRs were more flexible and diverse. This new ligand-proximal flexibility signature of GPCR activity was identified without knowledge of the ligand binding mode or previously defined switch regions, while being adjacent to the known transmission switch. Following this proof of concept, the ProFlex flexibility analysis coupled with pattern recognition and activity classification may be useful for predicting whether newly designed ligands behave as activators or inhibitors in protein families in general, based on the pattern of flexibility they induce in the protein.",
    "container-title": "Biomolecules",
    "DOI": "10.3390/biom10030454",
    "volume": "10",
    "issue": "3",
    "page": "454",
    "publisher": "MDPI AG",
    "title": "Machine Learning to Identify Flexibility Signatures of Class A GPCR Inhibition",
    "URL": "https://doi.org/ghm636",
    "PMCID": "PMC7175283",
    "PMID": "32183371",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.3390/biom10030454"
  },
  {
    "type": "article-journal",
    "id": "McKosnsZ",
    "author": [
      {
        "family": "Howard",
        "given": "Jeremy"
      },
      {
        "family": "Gugger",
        "given": "Sylvain"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          2,
          16
        ]
      ]
    },
    "abstract": "fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4–5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching.",
    "container-title": "Information",
    "DOI": "10.3390/info11020108",
    "volume": "11",
    "issue": "2",
    "page": "108",
    "publisher": "MDPI AG",
    "title": "Fastai: A Layered API for Deep Learning",
    "URL": "https://doi.org/ggmbms",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.3390/info11020108"
  },
  {
    "type": "article-journal",
    "id": "sLm8UD2q",
    "author": [
      {
        "family": "Raschka",
        "given": "Sebastian"
      },
      {
        "family": "Patterson",
        "given": "Joshua"
      },
      {
        "family": "Nolet",
        "given": "Corey"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2020,
          4,
          4
        ]
      ]
    },
    "abstract": "Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.",
    "container-title": "Information",
    "DOI": "10.3390/info11040193",
    "volume": "11",
    "issue": "4",
    "page": "193",
    "publisher": "MDPI AG",
    "title": "Machine Learning in Python: Main Developments and Technology Trends in Data Science, Machine Learning, and Artificial Intelligence",
    "URL": "https://doi.org/ghjtg8",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.3390/info11040193"
  },
  {
    "type": "article-journal",
    "id": "8xxCWPLQ",
    "author": [
      {
        "family": "Dillen",
        "given": "Mathias"
      },
      {
        "family": "Groom",
        "given": "Quentin"
      },
      {
        "family": "Agosti",
        "given": "Donat"
      },
      {
        "family": "Nielsen",
        "given": "Lars"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019,
          6,
          18
        ]
      ]
    },
    "container-title": "Biodiversity Information Science and Standards",
    "DOI": "10.3897/biss.3.37080",
    "volume": "3",
    "page": "e37080",
    "publisher": "Pensoft Publishers",
    "title": "Zenodo, an Archive and Publishing Repository: A tale of two herbarium specimen pilot projects",
    "URL": "https://doi.org/ghk2kn",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.3897/biss.3.37080"
  },
  {
    "type": "article-journal",
    "id": "Fzeo5SDl",
    "author": [
      {
        "family": "Singh",
        "given": "Jatinder"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2011
        ]
      ]
    },
    "container-title": "Journal of Pharmacology and Pharmacotherapeutics",
    "DOI": "10.4103/0976-500x.81919",
    "volume": "2",
    "issue": "2",
    "page": "138",
    "publisher": "Medknow",
    "title": "FigShare",
    "URL": "https://doi.org/cvqv67",
    "PMCID": "PMC3127351",
    "PMID": "21772785",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.4103/0976-500x.81919"
  },
  {
    "type": "article-journal",
    "id": "J91RXtV1",
    "author": [
      {
        "family": "Foster, MSLS",
        "given": "Erin D."
      },
      {
        "family": "Deardorff, MLIS",
        "given": "Ariel"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2017,
          4,
          4
        ]
      ]
    },
    "abstract": "The Open Science Framework (OSF) is a free, open source, research workflow web application developed and maintained by the Center for Open Science (COS).",
    "container-title": "Journal of the Medical Library Association",
    "DOI": "10.5195/jmla.2017.88",
    "volume": "105",
    "issue": "2",
    "publisher": "University Library System, University of Pittsburgh",
    "title": "Open Science Framework (OSF)",
    "URL": "https://doi.org/gfxvhq",
    "PMCID": "PMC5370619",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: doi:10.5195/jmla.2017.88"
  },
  {
    "id": "XPXTSaKX",
    "type": "book",
    "title": "PYTHON MACHINE LEARNING - THIRD EDITION: machine learning and deep learning with python, scikit ...-learn, and tensorflow 2.",
    "publisher": "PACKT Publishing Limited",
    "publisher-place": "Place of publication not identified",
    "source": "Open WorldCat",
    "event-place": "Place of publication not identified",
    "ISBN": "9781789955750",
    "note": "OCLC: 1109803766\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: isbn:9781789955750",
    "shortTitle": "PYTHON MACHINE LEARNING - THIRD EDITION",
    "language": "English",
    "author": [
      {
        "family": "RASCHKA",
        "given": "SEBASTIAN. MIRJALILI",
        "suffix": "VAHID"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    }
  },
  {
    "title": "The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets.",
    "volume": "10",
    "issue": "3",
    "page": "e0118432",
    "container-title": "PloS one",
    "container-title-short": "PLoS One",
    "ISSN": "1932-6203",
    "issued": {
      "date-parts": [
        [
          2015,
          3,
          4
        ]
      ]
    },
    "author": [
      {
        "given": "Takaya",
        "family": "Saito"
      },
      {
        "given": "Marc",
        "family": "Rehmsmeier"
      }
    ],
    "PMID": "25738806",
    "PMCID": "PMC4349800",
    "DOI": "10.1371/journal.pone.0118432",
    "abstract": "Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets. ",
    "URL": "https://www.ncbi.nlm.nih.gov/pubmed/25738806",
    "type": "article-journal",
    "id": "KnxQ4G8",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: pubmed:25738806"
  },
  {
    "id": "wgOFUxdw",
    "type": "article-journal",
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "container-title": "The Journal of Machine Learning Research",
    "page": "1929–1958",
    "volume": "15",
    "issue": "1",
    "source": "January 2014",
    "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
    "ISSN": "1532-4435",
    "shortTitle": "Dropout",
    "journalAbbreviation": "J. Mach. Learn. Res.",
    "author": [
      {
        "family": "Srivastava",
        "given": "Nitish"
      },
      {
        "family": "Hinton",
        "given": "Geoffrey"
      },
      {
        "family": "Krizhevsky",
        "given": "Alex"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      },
      {
        "family": "Salakhutdinov",
        "given": "Ruslan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2014",
          1,
          1
        ]
      ]
    },
    "URL": "http://dl.acm.org/citation.cfm?id=2670313",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: url:http://dl.acm.org/citation.cfm?id=2670313"
  },
  {
    "id": "eR3C2hhK",
    "type": "paper-conference",
    "title": "A simple weight decay can improve generalization",
    "container-title": "Proceedings of the 4th International Conference on Neural Information Processing Systems",
    "collection-title": "NIPS'91",
    "publisher": "Morgan Kaufmann Publishers Inc.",
    "publisher-place": "Denver, Colorado",
    "page": "950–957",
    "source": "ACM Digital Library",
    "event-place": "Denver, Colorado",
    "abstract": "It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.",
    "ISBN": "9781558602229",
    "author": [
      {
        "family": "Krogh",
        "given": "Anders"
      },
      {
        "family": "Hertz",
        "given": "John A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1991",
          12,
          2
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          2,
          16
        ]
      ]
    },
    "URL": "http://dl.acm.org/citation.cfm?id=2986916.2987033",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: url:http://dl.acm.org/citation.cfm?id=2986916.2987033"
  },
  {
    "id": "R1RpVu06",
    "type": "article-journal",
    "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
    "container-title": "Journal of Machine Learning Research",
    "page": "1929-1958",
    "volume": "15",
    "issue": "56",
    "source": "Journal of Machine Learning Research",
    "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different âthinnedâ networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
    "URL": "http://jmlr.org/papers/v15/srivastava14a.html",
    "shortTitle": "Dropout",
    "author": [
      {
        "family": "Srivastava",
        "given": "Nitish"
      },
      {
        "family": "Hinton",
        "given": "Geoffrey"
      },
      {
        "family": "Krizhevsky",
        "given": "Alex"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      },
      {
        "family": "Salakhutdinov",
        "given": "Ruslan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          2,
          17
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: url:http://jmlr.csail.mit.edu/papers/v15/srivastava14a.html"
  },
  {
    "id": "HKTnYDZq",
    "type": "webpage",
    "title": "American Society for Bioethics and Humanities",
    "abstract": "ASBH is an educational organization for professionals engaged in all endeavors related to clinical and academic bioethics and the health-related humanities.",
    "URL": "https://asbh.org/",
    "accessed": {
      "date-parts": [
        [
          "2021",
          2,
          17
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: url:https://asbh.org"
  },
  {
    "id": "pj5bK84R",
    "type": "book",
    "title": "Interpretable Machine Learning",
    "source": "christophm.github.io",
    "abstract": "Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.",
    "URL": "https://christophm.github.io/interpretable-ml-book/",
    "author": [
      {
        "family": "Molnar",
        "given": "Christoph"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2021",
          2,
          17
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: url:https://christophm.github.io/interpretable-ml-book"
  },
  {
    "id": "4oKcgKmU",
    "type": "paper-conference",
    "title": "Batch normalization: accelerating deep network training by reducing internal covariate shift",
    "container-title": "Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37",
    "collection-title": "ICML'15",
    "publisher": "JMLR.org",
    "publisher-place": "Lille, France",
    "page": "448–456",
    "source": "ACM Digital Library",
    "event-place": "Lille, France",
    "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.",
    "shortTitle": "Batch normalization",
    "author": [
      {
        "family": "Ioffe",
        "given": "Sergey"
      },
      {
        "family": "Szegedy",
        "given": "Christian"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2015",
          7,
          6
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          2,
          16
        ]
      ]
    },
    "URL": "https://dl.acm.org/citation.cfm?id=3045118.3045167",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: url:https://dl.acm.org/citation.cfm?id=3045118.3045167"
  },
  {
    "id": "jdSXX5Vn",
    "type": "paper-conference",
    "title": "How transferable are features in deep neural networks?",
    "container-title": "Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2",
    "collection-title": "NIPS'14",
    "publisher": "MIT Press",
    "publisher-place": "Montreal, Canada",
    "page": "3320–3328",
    "source": "ACM Digital Library",
    "event-place": "Montreal, Canada",
    "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.",
    "author": [
      {
        "family": "Yosinski",
        "given": "Jason"
      },
      {
        "family": "Clune",
        "given": "Jeff"
      },
      {
        "family": "Bengio",
        "given": "Yoshua"
      },
      {
        "family": "Lipson",
        "given": "Hod"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2014",
          12,
          8
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          2,
          16
        ]
      ]
    },
    "URL": "https://dl.acm.org/doi/abs/10.5555/2969033.2969197",
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: url:https://dl.acm.org/doi/abs/10.5555/2969033.2969197"
  },
  {
    "type": "webpage",
    "URL": "https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#reproducibility",
    "title": "Deep Learning SDK Documentation",
    "issued": {
      "date-parts": [
        [
          2018,
          11,
          1
        ]
      ]
    },
    "author": [
      {
        "literal": "NVIDIA"
      }
    ],
    "note": "This CSL JSON Item was loaded by Manubot v0.4.1 from a manual reference file.\nmanual_reference_filename: manual-references.json\nstandard_id: url:https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#reproducibility",
    "id": "1GSwNJdl7"
  },
  {
    "type": "webpage",
    "URL": "https://github.com/Benjamin-Lee/deep-rules",
    "title": "Benjamin-Lee/deep-rules GitHub repository",
    "container-title": "GitHub",
    "issued": {
      "date-parts": [
        [
          2018
        ]
      ]
    },
    "author": [
      {
        "given": "Benjamin",
        "family": "Lee"
      }
    ],
    "note": "This CSL JSON Item was loaded by Manubot v0.4.1 from a manual reference file.\nmanual_reference_filename: manual-references.json\nstandard_id: url:https://github.com/Benjamin-Lee/deep-rules",
    "id": "ysdRl4lj"
  },
  {
    "id": "ndSzNxZQ",
    "type": "book",
    "title": "apple/turicreate",
    "publisher": "Apple",
    "genre": "C++",
    "source": "GitHub",
    "abstract": "Turi Create simplifies the development of custom machine learning models.",
    "URL": "https://github.com/apple/turicreate",
    "note": "original-date: 2017-12-01T00:42:04Z\nThis CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: url:https://github.com/apple/turicreate",
    "issued": {
      "date-parts": [
        [
          "2021",
          2,
          17
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          2,
          17
        ]
      ]
    }
  },
  {
    "id": "fMQbR11C",
    "type": "webpage",
    "title": "Keras: the Python deep learning API",
    "abstract": "Keras documentation",
    "URL": "https://keras.io/",
    "accessed": {
      "date-parts": [
        [
          "2021",
          2,
          17
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: url:https://keras.io"
  },
  {
    "id": "mIx19cpn",
    "type": "paper-conference",
    "title": "Phonetic Classification and Recognition Using the Multi-Layer Perceptron",
    "container-title": "Advances in Neural Information Processing Systems",
    "publisher": "Morgan-Kaufmann",
    "volume": "3",
    "source": "Neural Information Processing Systems",
    "URL": "https://proceedings.neurips.cc/paper/1990/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Paper.pdf",
    "author": [
      {
        "family": "Leung",
        "given": "Hong"
      },
      {
        "family": "Glass",
        "given": "James"
      },
      {
        "family": "Phillips",
        "given": "Michael"
      },
      {
        "family": "Zue",
        "given": "Victor W."
      }
    ],
    "editor": [
      {
        "family": "Lippmann",
        "given": "R. P."
      },
      {
        "family": "Moody",
        "given": "J."
      },
      {
        "family": "Touretzky",
        "given": "D."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1991"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          2,
          17
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: url:https://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning"
  },
  {
    "id": "cl8ts1jx",
    "type": "webpage",
    "title": "10 organizations leading the way in ethical AI — SAGE Ocean | Big Data, New Tech, Social Science",
    "URL": "https://web.archive.org/web/20210112231619/https://ocean.sagepub.com/blog/10-organizations-leading-the-way-in-ethical-ai",
    "issued": {
      "date-parts": [
        [
          "2021",
          1,
          12
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          2,
          17
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: url:https://web.archive.org/web/20210112231619/https://ocean.sagepub.com/blog/10-organizations-leading-the-way-in-ethical-ai"
  },
  {
    "id": "16qsznKWN",
    "type": "webpage",
    "title": "Artificial Intelligence, Ethics, and Society — Home",
    "URL": "https://www.aies-conference.com/2021/",
    "language": "en-US",
    "accessed": {
      "date-parts": [
        [
          "2021",
          2,
          17
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: url:https://www.aies-conference.com"
  },
  {
    "id": "TXT7Jt5l",
    "type": "article-journal",
    "title": "Scikit-learn: Machine Learning in Python",
    "container-title": "Journal of Machine Learning Research",
    "page": "2825-2830",
    "volume": "12",
    "issue": "85",
    "source": "www.jmlr.org",
    "URL": "http://jmlr.org/papers/v12/pedregosa11a.html",
    "ISSN": "1533-7928",
    "shortTitle": "Scikit-learn",
    "author": [
      {
        "family": "Pedregosa",
        "given": "Fabian"
      },
      {
        "family": "Varoquaux",
        "given": "Gaël"
      },
      {
        "family": "Gramfort",
        "given": "Alexandre"
      },
      {
        "family": "Michel",
        "given": "Vincent"
      },
      {
        "family": "Thirion",
        "given": "Bertrand"
      },
      {
        "family": "Grisel",
        "given": "Olivier"
      },
      {
        "family": "Blondel",
        "given": "Mathieu"
      },
      {
        "family": "Prettenhofer",
        "given": "Peter"
      },
      {
        "family": "Weiss",
        "given": "Ron"
      },
      {
        "family": "Dubourg",
        "given": "Vincent"
      },
      {
        "family": "Vanderplas",
        "given": "Jake"
      },
      {
        "family": "Passos",
        "given": "Alexandre"
      },
      {
        "family": "Cournapeau",
        "given": "David"
      },
      {
        "family": "Brucher",
        "given": "Matthieu"
      },
      {
        "family": "Perrot",
        "given": "Matthieu"
      },
      {
        "family": "Duchesnay",
        "given": "Édouard"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2011"
        ]
      ]
    },
    "accessed": {
      "date-parts": [
        [
          "2021",
          2,
          17
        ]
      ]
    },
    "note": "This CSL JSON Item was automatically generated by Manubot v0.4.1 using citation-by-identifier.\nstandard_id: url:https://www.jmlr.org/papers/v12/pedregosa11a.html"
  }
]
